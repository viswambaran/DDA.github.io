<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 5 Linear &amp; Polynomial Regression | Module Notes</title>
  <meta name="description" content="Module notes for Data Driven Analytics." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 5 Linear &amp; Polynomial Regression | Module Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Module notes for Data Driven Analytics." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 5 Linear &amp; Polynomial Regression | Module Notes" />
  
  <meta name="twitter:description" content="Module notes for Data Driven Analytics." />
  

<meta name="author" content="Dylan Viswambaran" />


<meta name="date" content="2022-06-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-with-logistic-regression.html"/>
<link rel="next" href="probabilistic-classifiers-and-gradient-descent.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Driven Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Driven Analytics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#data-structures-in-python"><i class="fa fa-check"></i><b>1.1</b> Data Structures in Python</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#structured-vs-unstructured-data"><i class="fa fa-check"></i><b>1.2</b> Structured vs Unstructured data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Driven Pipelines</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#example-of-a-data-driven-pipeline-using-the-iris-dataset"><i class="fa fa-check"></i>Example of a data driven pipeline using the iris dataset</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-acquisition"><i class="fa fa-check"></i>Data acquisition</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning"><i class="fa fa-check"></i>Data cleaning</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-exploration"><i class="fa fa-check"></i>Data exploration</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-modelling"><i class="fa fa-check"></i>Data modelling</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-visualisation"><i class="fa fa-check"></i>Data visualisation</a></li>
</ul></li>
<li class="chapter" data-level="2.1" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning-1"><i class="fa fa-check"></i><b>2.1</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#how-do-we-clean-data"><i class="fa fa-check"></i>How do we clean data?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours (KNN)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#key-information"><i class="fa fa-check"></i><b>3.1</b> Key information</a></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages-and-disadvantages-of-using-the-knn-model"><i class="fa fa-check"></i><b>3.2</b> Advantages and Disadvantages of using the KNN model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages"><i class="fa fa-check"></i><b>3.2.1</b> Advantages</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#disadvantages"><i class="fa fa-check"></i><b>3.2.2</b> Disadvantages</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#things-to-guide-you-as-you-choose-the-value-of-k"><i class="fa fa-check"></i><b>3.2.3</b> Things to guide you as you choose the value of K</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#alternate-distance-metrics-to-euclidean"><i class="fa fa-check"></i><b>3.2.4</b> Alternate distance metrics to Euclidean</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#knn-code-example"><i class="fa fa-check"></i><b>3.2.5</b> KNN code example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Classification with Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#logistic-regression-assumptions"><i class="fa fa-check"></i><b>4.0.1</b> Logistic regression assumptions</a></li>
<li class="chapter" data-level="4.0.2" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#advantages-and-disadvantages-of-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.2</b> Advantages and Disadvantages of using Logistic Regression</a></li>
<li class="chapter" data-level="4.0.3" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#code-example-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.3</b> Code example using Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Linear &amp; Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#linear-regression"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#assumptions"><i class="fa fa-check"></i><b>5.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.3" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#classification-vs-regression"><i class="fa fa-check"></i><b>5.3</b> Classification vs Regression</a></li>
<li class="chapter" data-level="5.4" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-of-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Code example of Linear Regression</a></li>
<li class="chapter" data-level="5.5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-with-polynomial-regression"><i class="fa fa-check"></i><b>5.6</b> Code example with Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html"><i class="fa fa-check"></i><b>6</b> Probabilistic Classifiers and Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#naive-bayes-classification"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes Classification</a></li>
<li class="chapter" data-level="6.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-naive-bayes-classifier"><i class="fa fa-check"></i><b>6.2</b> Types of Naive Bayes Classifier</a></li>
<li class="chapter" data-level="6.3" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-and-disadvantages-of-naive-bayes-classifers"><i class="fa fa-check"></i><b>6.3</b> Advantages and Disadvantages of Naive Bayes Classifers</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-2"><i class="fa fa-check"></i><b>6.3.1</b> Advantages</a></li>
<li class="chapter" data-level="6.3.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-2"><i class="fa fa-check"></i><b>6.3.2</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#multinomial-naive-bayes"><i class="fa fa-check"></i><b>6.4</b> Multinomial Naive Bayes</a></li>
<li class="chapter" data-level="6.5" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#gradient-descent"><i class="fa fa-check"></i><b>6.5</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-gradient-descent"><i class="fa fa-check"></i><b>6.5.1</b> Types of Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html"><i class="fa fa-check"></i><b>7</b> Model Evaluation for Regression and Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>7.1</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="7.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#issues-with-train-test-split"><i class="fa fa-check"></i><b>7.2.1</b> Issues with train-test-split</a></li>
<li class="chapter" data-level="7.2.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#cross-validation"><i class="fa fa-check"></i><b>7.2.2</b> Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#measuring-model-performance"><i class="fa fa-check"></i><b>7.3</b> Measuring Model Performance</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#variance-and-sensitivity"><i class="fa fa-check"></i><b>7.3.1</b> Variance and sensitivity</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>7.3.2</b> Confusion Matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html"><i class="fa fa-check"></i><b>8</b> Advanced Statistical Learning Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#unsupervised-learning"><i class="fa fa-check"></i><b>8.1</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#clustering"><i class="fa fa-check"></i><b>8.1.1</b> Clustering</a></li>
<li class="chapter" data-level="8.1.2" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#decision-trees"><i class="fa fa-check"></i><b>8.1.2</b> Decision Trees</a></li>
<li class="chapter" data-level="8.1.3" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#principal-component-analysis"><i class="fa fa-check"></i><b>8.1.3</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="8.1.4" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#support-vector-machine"><i class="fa fa-check"></i><b>8.1.4</b> Support Vector Machine</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-database-systems.html"><a href="relational-database-systems.html"><i class="fa fa-check"></i><b>9</b> Relational Database Systems</a>
<ul>
<li class="chapter" data-level="9.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#sqlite"><i class="fa fa-check"></i><b>9.1</b> SQLite</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>9.1.1</b> Advantages and Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Module Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-polynomial-regression" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Module 5</span> Linear &amp; Polynomial Regression<a href="linear-polynomial-regression.html#linear-polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="linear-regression" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Linear Regression<a href="linear-polynomial-regression.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>Linear regression is a supervised learning algorithm that compares input (X) and output (Y) variables based on labeled data. It’s used for finding the relationship between the two variables and predicting future results based on past relationships.</p>
</blockquote>
<p>Completing a simple linear regression on a set of data results in a line on a plot representing the relationship between the independent variable X and the dependent variable Y. The simple linear regression predicts the value of the dependent variable based on the independent variable.</p>
</div>
<div id="assumptions" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Assumptions<a href="linear-polynomial-regression.html#assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><strong>Linearity and additivity:</strong> The expected value of the dependent variable is a straight-line function of the independent variable. The effects of different independent variables are additive to the expected value of the dependent variable. <strong>Check this using a scatterplot.</strong></p></li>
<li><p><strong>No significant outliers</strong>: Outliers can have a negative effect on the regression analysis.</p></li>
<li><p><strong>Statistical independence:</strong> There is no correlation between consecutive errors when using time series data. The observations are independent of each other.</p></li>
<li><p><strong>Homoscedasticity:</strong> The errors have a constant variance in time compared to predictions and when compared to any independent variable.</p></li>
<li><p><strong>Normality:</strong> For a fixed value of X, Y values are distributed normally.</p></li>
</ul>
</div>
<div id="classification-vs-regression" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Classification vs Regression<a href="linear-polynomial-regression.html#classification-vs-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Differences:</p>
<ul>
<li>Regression output is continuous (numerical)<br />
</li>
<li>Classification output is discrete (categorical)</li>
</ul>
</div>
<div id="code-example-of-linear-regression" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Code example of Linear Regression<a href="linear-polynomial-regression.html#code-example-of-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<pre><code>**Building a linear regression model**
We’ll import the numpy and pandas library in the Jupyter notebook and read the data
using pandas.
&quot;&quot;&quot;
import numpy as np
import pandas as pd
# Read the given CSV file, and view some sample records
advertising = pd.read_csv(&quot;Company_data.csv&quot;)
&quot;&quot;&quot;The dataset looks like this. Here our target variable is the Sales column.&quot;&quot;&quot;
advertising.head(10)
&quot;&quot;&quot;Let’s perform some tasks to understand the data like shape, info, and 
describe.&quot;&quot;&quot;
# Shape of our dataset
advertising.shape
# Info our dataset
advertising.info()
# Describe our dataset
advertising.describe()
&quot;&quot;&quot;Let’s now visualize the data using the matplolib and seaborn library. We’ll make
a pairplot of all the columns and see which columns are the most correlated to 
Sales.&quot;&quot;&quot;
# Import matplotlib and seaborn libraries to visualize the data
import matplotlib.pyplot as plt 
import seaborn as sns
# Using pairplot we&#39;ll visualize the data for correlation
sns.pairplot(advertising, x_vars=[&#39;TV&#39;, &#39;Radio&#39;,&#39;Newspaper&#39;], 
             y_vars=&#39;Sales&#39;, height=4, aspect=1, kind=&#39;scatter&#39;)
plt.show()
&quot;&quot;&quot;If we cannot determine the correlation using a scatter plot, we can use the 
seaborn heatmap to visualize the data.
&quot;&quot;&quot;
# Visualizing the data using heatmap
sns.heatmap(advertising.corr(), cmap=&quot;YlGnBu&quot;, annot = True)
plt.show()
&quot;&quot;&quot;As we can see from the above graphs, the TV column seems most correlated to 
Sales.
Let’s perform the simple linear regression model using TV as our feature variable.
Equation of simple linear regression
y = c + mX
In our case:
y = c + m * TV
The m values are known as model coefficients or model parameters.
We’ll perform simple linear regression in four steps.
* Create X and y
* Create Train and Test set
* Train your model
* Evaluate the model
The independent variable represents X, and y represents the target variable in a 
simple linear regression model.
&quot;&quot;&quot;
X = advertising[[&#39;TV&#39;]]
y = advertising[[&#39;Sales&#39;]]
&quot;&quot;&quot;We need to split our variables into training and testing sets. Using the 
training set, we’ll build the model and perform the model on the testing set. We’ll
divide the training and testing sets into a 7:3 ratio, respectively.
We’ll split the data by importing train_test_split from the sklearn.model_selection
library.
&quot;&quot;&quot;
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X,y)
&quot;&quot;&quot;To retrieve the intercept:
&quot;&quot;&quot;
print(regressor.intercept_)
&quot;&quot;&quot;For retrieving the slope (coefficient of x):
&quot;&quot;&quot;
print(regressor.coef_)
&quot;&quot;&quot;We can conlude that our model is:
* y = 6.97 + 0.05 * TV
That means that if we use TV=100, then:
* y = 6.97 + 0.05 * 100 = 6.97 + 5 = 11.97
Lets extract the first record e.g. record 0
&quot;&quot;&quot;
advertising.head(1)
&quot;&quot;&quot;And now, let&#39;s try to see the prediction for **TV value: 230.1**, as we can see 
our prediction is 19.73, that is close to 22.1&quot;&quot;&quot;
y_pred = regressor.predict([[230.1]])
y_pred</code></pre>
</div>
<div id="polynomial-regression" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Polynomial Regression<a href="linear-polynomial-regression.html#polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>Polynomial regression is a form of linear regression in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial.</p>
</blockquote>
<ul>
<li><p>Polynomial Regression is a form of regression analysis in which the relationship between the independent variables and dependent variables are modelled in the nth degree polynomial.</p></li>
<li><p>Polynomial Regression models are usually fitted with the method of least squares. The least-square method minimizes the variance of the coefficients, under the Gauss Markov Theorem.</p></li>
<li><p>Polynomial Regression is a special case of Linear Regression where we fit the polynomial equation on the data with a curvilinear relationship between the dependent and independent variables.</p></li>
</ul>
</div>
<div id="code-example-with-polynomial-regression" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Code example with Polynomial regression<a href="linear-polynomial-regression.html#code-example-with-polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<pre><code>
# **PART 2: Using Polynomial Regression**
**Question 1.1:** Load the *Years_salary_data.csv* file in the Colab and *read* it 
as a dataframe.
Print and examine the dataset.
&quot;&quot;&quot;
# Provide your solution here
import pandas as pd
data_ys = pd.read_csv(&#39;Years_salary_data.csv&#39;)
data_ys.head()
&quot;&quot;&quot;**Question 1.2:** Let&#39;s plot a scatterplot to examine the data.&quot;&quot;&quot;
sns.scatterplot(x=&#39;Years&#39;, y=&#39;Salary&#39;, data=data_ys)
&quot;&quot;&quot;**Question 2:** Plot the years-salary data using a lmplot. Use in **x-axis** the
**Years** and as **y-axis** the **Salary**. &quot;&quot;&quot;
# Provide your solution here
sns.lmplot(x=&#39;Years&#39;, y=&#39;Salary&#39;, data=data_ys, height=10)
&quot;&quot;&quot;**Question 3:** Create the **X (Years)** and **y (Salary)** variables of your 
dataset.&quot;&quot;&quot;
# Provide your solution here
X= data_ys[[&#39;Years&#39;]]
y= data_ys[[&#39;Salary&#39;]]
&quot;&quot;&quot;**Question 4:** Create the Linear Regression model. Fit **X** and **y**.
&quot;&quot;&quot;
# Provide your solution here
from sklearn.linear_model import LinearRegression 
lin = LinearRegression() 
  
lin.fit(X, y)
&quot;&quot;&quot;**Question 5:** Create the Polynomial Regression model (degree=8). Fit X and y.
Study and run the following code.
&quot;&quot;&quot;
# Fitting Polynomial Regression to the dataset 
from sklearn.preprocessing import PolynomialFeatures 
  
poly = PolynomialFeatures(degree = 8) 
X_poly = poly.fit_transform(X) 
  
poly.fit(X_poly, y) 
lin2 = LinearRegression() 
lin2.fit(X_poly, y)
&quot;&quot;&quot;**Question 6:** Plot the Linear Regression model.
 Study and run the following code. Is the Regression line a good fit?
&quot;&quot;&quot;
# Visualising the Linear Regression results 
plt.scatter(X, y, color = &#39;blue&#39;) 
  
plt.plot(X, lin.predict(X), color = &#39;red&#39;) 
plt.title(&#39;Linear Regression&#39;) 
plt.xlabel(&#39;Temperature&#39;) 
plt.ylabel(&#39;Pressure&#39;) 
  
plt.show() 
# The regression line does not represent the data
&quot;&quot;&quot;**Question 7:** Plot the Polynomial Regression results. 
Study and run the following code. Is the Polynomial line a good fit?
&quot;&quot;&quot;
# Visualising the Polynomial Regression results 
plt.scatter(X, y, color = &#39;blue&#39;) 
  
plt.plot(X, lin2.predict(poly.fit_transform(X)), color = &#39;red&#39;) 
plt.title(&#39;Polynomial Regression&#39;) 
# The Polynomial line represent better the data!
&quot;&quot;&quot;**Question 8:** Let us change the degree of the Polynomial line.
Study and run the following code using the following.
*   degree=4
*   degree=10
&quot;&quot;&quot;
# Fitting Polynomial Regression to the dataset 
from sklearn.preprocessing import PolynomialFeatures 
  
poly = PolynomialFeatures(degree = 10) 
X_poly = poly.fit_transform(X) 
  
poly.fit(X_poly, y) 
lin2 = LinearRegression() 
lin2.fit(X_poly, y) 
# Visualising the Polynomial Regression results 
plt.scatter(X, y, color = &#39;blue&#39;) 
  
plt.plot(X, lin2.predict(poly.fit_transform(X)), color = &#39;red&#39;) 
plt.title(&#39;Polynomial Regression&#39;)
&quot;&quot;&quot;&gt; The degree affects the fit of our Polynomial line, if the degree is very small
our model does not fit well to the data, if the degree is very high the model 
overfits the data. We need to find a balance to control it.
**Question 9:** Create a Polynomial Regression model using degree 11. Use the code 
from the previous examples.
&quot;&quot;&quot;
# Provide your solution here
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=11)
X_poly = poly_reg.fit_transform(X)
pol_reg = LinearRegression()
pol_reg.fit(X_poly, y)
&quot;&quot;&quot;**Question 10:** Predict the salary for years of experience as follows.
*   32.5
*   20.5
Create a linear and Polynomial Regression prediction. 
Study and run the following code.
Which model is more accurate?
&quot;&quot;&quot;
# Predicting a new result with Linear Regression
print(lin.predict([[20.5]]))
# Predicting a new result with Polymonial Regression
print(pol_reg.predict(poly_reg.fit_transform([[20.5]])))
# Answer: __Polynomial Regression is a better fit model in this dataset___</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-with-logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="probabilistic-classifiers-and-gradient-descent.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-Module5.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
