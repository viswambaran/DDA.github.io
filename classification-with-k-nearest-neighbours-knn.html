<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 3 Classification with K-Nearest Neighbours (KNN) | Module Notes</title>
  <meta name="description" content="Module notes for Data Driven Analytics." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 3 Classification with K-Nearest Neighbours (KNN) | Module Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Module notes for Data Driven Analytics." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 3 Classification with K-Nearest Neighbours (KNN) | Module Notes" />
  
  <meta name="twitter:description" content="Module notes for Data Driven Analytics." />
  

<meta name="author" content="Dylan Viswambaran" />


<meta name="date" content="2022-06-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-data-driven-pipelines.html"/>
<link rel="next" href="classification-with-logistic-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Driven Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Driven Analytics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#data-structures-in-python"><i class="fa fa-check"></i><b>1.1</b> Data Structures in Python</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#structured-vs-unstructured-data"><i class="fa fa-check"></i><b>1.2</b> Structured vs Unstructured data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Driven Pipelines</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#example-of-a-data-driven-pipeline-using-the-iris-dataset"><i class="fa fa-check"></i>Example of a data driven pipeline using the iris dataset</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-acquisition"><i class="fa fa-check"></i>Data acquisition</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning"><i class="fa fa-check"></i>Data cleaning</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-exploration"><i class="fa fa-check"></i>Data exploration</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-modelling"><i class="fa fa-check"></i>Data modelling</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-visualisation"><i class="fa fa-check"></i>Data visualisation</a></li>
</ul></li>
<li class="chapter" data-level="2.1" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning-1"><i class="fa fa-check"></i><b>2.1</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#how-do-we-clean-data"><i class="fa fa-check"></i>How do we clean data?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours (KNN)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#key-information"><i class="fa fa-check"></i><b>3.1</b> Key information</a></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages-and-disadvantages-of-using-the-knn-model"><i class="fa fa-check"></i><b>3.2</b> Advantages and Disadvantages of using the KNN model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages"><i class="fa fa-check"></i><b>3.2.1</b> Advantages</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#disadvantages"><i class="fa fa-check"></i><b>3.2.2</b> Disadvantages</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#things-to-guide-you-as-you-choose-the-value-of-k"><i class="fa fa-check"></i><b>3.2.3</b> Things to guide you as you choose the value of K</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#alternate-distance-metrics-to-euclidean"><i class="fa fa-check"></i><b>3.2.4</b> Alternate distance metrics to Euclidean</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#knn-code-example"><i class="fa fa-check"></i><b>3.2.5</b> KNN code example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Classification with Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#logistic-regression-assumptions"><i class="fa fa-check"></i><b>4.0.1</b> Logistic regression assumptions</a></li>
<li class="chapter" data-level="4.0.2" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#advantages-and-disadvantages-of-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.2</b> Advantages and Disadvantages of using Logistic Regression</a></li>
<li class="chapter" data-level="4.0.3" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#code-example-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.3</b> Code example using Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Linear &amp; Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#linear-regression"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#assumptions"><i class="fa fa-check"></i><b>5.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.3" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#classification-vs-regression"><i class="fa fa-check"></i><b>5.3</b> Classification vs Regression</a></li>
<li class="chapter" data-level="5.4" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-of-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Code example of Linear Regression</a></li>
<li class="chapter" data-level="5.5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-with-polynomial-regression"><i class="fa fa-check"></i><b>5.6</b> Code example with Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html"><i class="fa fa-check"></i><b>6</b> Probabilistic Classifiers and Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#naive-bayes-classification"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes Classification</a></li>
<li class="chapter" data-level="6.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-naive-bayes-classifier"><i class="fa fa-check"></i><b>6.2</b> Types of Naive Bayes Classifier</a></li>
<li class="chapter" data-level="6.3" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-and-disadvantages-of-naive-bayes-classifers"><i class="fa fa-check"></i><b>6.3</b> Advantages and Disadvantages of Naive Bayes Classifers</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-2"><i class="fa fa-check"></i><b>6.3.1</b> Advantages</a></li>
<li class="chapter" data-level="6.3.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-2"><i class="fa fa-check"></i><b>6.3.2</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#multinomial-naive-bayes"><i class="fa fa-check"></i><b>6.4</b> Multinomial Naive Bayes</a></li>
<li class="chapter" data-level="6.5" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#gradient-descent"><i class="fa fa-check"></i><b>6.5</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-gradient-descent"><i class="fa fa-check"></i><b>6.5.1</b> Types of Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html"><i class="fa fa-check"></i><b>7</b> Model Evaluation for Regression and Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>7.1</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="7.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#issues-with-train-test-split"><i class="fa fa-check"></i><b>7.2.1</b> Issues with train-test-split</a></li>
<li class="chapter" data-level="7.2.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#cross-validation"><i class="fa fa-check"></i><b>7.2.2</b> Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#measuring-model-performance"><i class="fa fa-check"></i><b>7.3</b> Measuring Model Performance</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#variance-and-sensitivity"><i class="fa fa-check"></i><b>7.3.1</b> Variance and sensitivity</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>7.3.2</b> Confusion Matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html"><i class="fa fa-check"></i><b>8</b> Advanced Statistical Learning Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#unsupervised-learning"><i class="fa fa-check"></i><b>8.1</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#clustering"><i class="fa fa-check"></i><b>8.1.1</b> Clustering</a></li>
<li class="chapter" data-level="8.1.2" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#decision-trees"><i class="fa fa-check"></i><b>8.1.2</b> Decision Trees</a></li>
<li class="chapter" data-level="8.1.3" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#principal-component-analysis"><i class="fa fa-check"></i><b>8.1.3</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="8.1.4" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#support-vector-machine"><i class="fa fa-check"></i><b>8.1.4</b> Support Vector Machine</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-database-systems.html"><a href="relational-database-systems.html"><i class="fa fa-check"></i><b>9</b> Relational Database Systems</a>
<ul>
<li class="chapter" data-level="9.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#sqlite"><i class="fa fa-check"></i><b>9.1</b> SQLite</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>9.1.1</b> Advantages and Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Module Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-with-k-nearest-neighbours-knn" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Module 3</span> Classification with K-Nearest Neighbours (KNN)<a href="classification-with-k-nearest-neighbours-knn.html#classification-with-k-nearest-neighbours-knn" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<blockquote>
<p>The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.</p>
</blockquote>
<div id="key-information" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Key information<a href="classification-with-k-nearest-neighbours-knn.html#key-information" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A KNN model calculates similarity using the distance between two points on a graph. The greater the distance between the points, the less similar they are. There are multiple ways of calculating the distance between points, but the most common distance metric is just <strong>Euclidean distance</strong> (the distance between two points in a straight line).</p>
<p>KNN is a <strong>supervised learning</strong> algorithm, meaning that the examples in the dataset must have labels assigned to them/their classes must be known. There are two other important things to know about KNN. First, KNN is a non-parametric algorithm. This means that no assumptions about the dataset are made when the model is used. Rather, the model is constructed entirely from the provided data. Second, there is no splitting of the dataset into training and test sets when using KNN. KNN makes no generalizations between a training and testing set, so all the training data is also used when the model is asked to make predictions.</p>
</div>
<div id="advantages-and-disadvantages-of-using-the-knn-model" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Advantages and Disadvantages of using the KNN model<a href="classification-with-k-nearest-neighbours-knn.html#advantages-and-disadvantages-of-using-the-knn-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="advantages" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Advantages<a href="classification-with-k-nearest-neighbours-knn.html#advantages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><strong>Easy to implement:</strong> Given the algorithm’s simplicity and accuracy, it is one of the first classifiers that a new data scientist will learn.</p></li>
<li><p><strong>Adapts easily:</strong> As new training samples are added, the algorithm adjusts to account for any new data since all training data is stored into memory.</p></li>
<li><p><strong>Few hyperparameters:</strong> KNN only requires a k value and a distance metric, which is low when compared to other machine learning algorithms.</p></li>
</ul>
</div>
<div id="disadvantages" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Disadvantages<a href="classification-with-k-nearest-neighbours-knn.html#disadvantages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><strong>Does not scale well:</strong> Since KNN is a lazy algorithm, it takes up more memory and data storage compared to other classifiers. This can be costly from both a time and money perspective. More memory and storage will drive up business expenses and more data can take longer to compute. While different data structures, such as Ball-Tree, have been created to address the computational inefficiencies, a different classifier may be ideal depending on the business problem.</p></li>
<li><p><strong>Curse of dimensionality:</strong> The KNN algorithm tends to fall victim to the curse of dimensionality, which means that it doesn’t perform well with high-dimensional data inputs. This is sometimes also referred to as the peaking phenomenon (PDF, 340 MB) (link resides outside of ibm.com), where after the algorithm attains the optimal number of features, additional features increases the amount of classification errors, especially when the sample size is smaller.</p></li>
<li><p><strong>Prone to overfitting:</strong> Due to the “curse of dimensionality”, KNN is also more prone to overfitting. While feature selection and dimensionality reduction techniques are leveraged to prevent this from occurring, the value of k can also impact the model’s behavior. Lower values of k can overfit the data, whereas higher values of k tend to “smooth out” the prediction values since it is averaging the values over a greater area, or neighborhood. However, if the value of k is too high, then it can underfit the data.</p></li>
</ul>
</div>
<div id="things-to-guide-you-as-you-choose-the-value-of-k" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Things to guide you as you choose the value of K<a href="classification-with-k-nearest-neighbours-knn.html#things-to-guide-you-as-you-choose-the-value-of-k" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>As K approaches 1, your prediction becomes less stable.<br />
</li>
<li>As your value of K increases, your prediction becomes more stable.<br />
</li>
<li>When you start receiving an increasing number of errors, you should know you are pushing your K too far.<br />
</li>
<li>Taking a majority vote among labels needs K to be an odd number to have a tiebreaker.</li>
</ul>
</div>
<div id="alternate-distance-metrics-to-euclidean" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Alternate distance metrics to Euclidean<a href="classification-with-k-nearest-neighbours-knn.html#alternate-distance-metrics-to-euclidean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Minkowski distance<br />
</li>
<li>Manhattan distance<br />
</li>
<li>Hamming distance<br />
</li>
<li>Cosine distance</li>
</ul>
</div>
<div id="knn-code-example" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> KNN code example<a href="classification-with-k-nearest-neighbours-knn.html#knn-code-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<pre><code> **PART 2: Data Modelling**
**Question 7:** We will need to split our dataset into feature data (X) and target 
data (y). As discussed in class the feature data is the data that our machine 
learning model will *learn* in order to classify according to the target variable. 
These are:
*  sepal.length
*  sepal.width
*  petal.length
*  petal.width
Extract the data for the first four columns and store it in a new dataframe called 
*iris_data_feature_data*.
&quot;&quot;&quot;
# Provide your solution here
iris_data_feature_data = 
iris_data[[&#39;sepal.length&#39;,&#39;sepal.width&#39;,&#39;petal.length&#39;,&#39;petal.width&#39;]]
iris_data_feature_data
df1 = iris_data.iloc[:, 0:4]
df1
&quot;&quot;&quot;**Question 8:** Now we will need to create the *iris_data_target_data* 
dataframe. The new dataframe will store only the target values.&quot;&quot;&quot;
# Provide your solution here
iris_data_target_data = iris_data[[&#39;variety&#39;]]
iris_data_target_data
&quot;&quot;&quot;**Question 9:** Study and run the following code.&quot;&quot;&quot;
X_count = iris_data_feature_data.count() 
y_count = iris_data_target_data.count()
print(&quot;X counts&quot;)
print(X_count)
print(&quot;y count&quot;)
print(y_count)
&quot;&quot;&quot;&gt; There are 150 rows for both *iris_data_feature_data* and 
*iris_data_target_data*. Note, that we cannot proceed with modeling if the counts 
do not match as we will receive an error! All X data should have the same number of
rows as the y data.
**Question 10:** As seen in class, create an X and y variable to represent 
iris_data_feature_data and iris_data_target_data respectively. 
Here, we just create two variables for representing X and y for our model.
&quot;&quot;&quot;
# Provide your solution here
X = iris_data_feature_data
y = iris_data_target_data
&quot;&quot;&quot;**Question 11:** Create your KNN model and fit it using the X and y variables 
from the previous step. Use the *KNeighborsClassifier* including 4 neighbors.&quot;&quot;&quot;
# Provide your solution here
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X, y)
&quot;&quot;&quot;&gt; The output of this script provides some details about the default setup of the
KNN model.
**Question 12:** What is the *metric=&#39;minkowski&#39;*? Look online and keep a note of 
your findings.
&quot;&quot;&quot;
# Provide your solution here
# The default metric is minkowski, and with p=2 is equivalent to the standard 
Euclidean metric. 
# More details: 
https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClass
ifier.html
&quot;&quot;&quot;**Question 13:** Provide a script to *predict* the class of a flower with the 
following data:
*  sepal.length = 4.5
*  sepal.width = 3.2
*  petal.length = 1.1
*  petal.width = 0.4
&quot;&quot;&quot;
# Examine the following code
knn.predict([[4.5, 3.2, 1.1, 0.4]])
&quot;&quot;&quot;**Question 14:** Print the data for row 4 and compare with the prediction of the
previous step. Then answer the questions.&quot;&quot;&quot;
# Provide your solution here
iris_data.loc[4]
# Compare data from question 13 and data for row 4, Do they look similar? 
# Yes! Row:4 5.0 3.6 1.4 0.2 Setosa
# Is the prediction of question 13 good?
# Yes, data look close to each other!
</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-data-driven-pipelines.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-with-logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/03-Module3.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
