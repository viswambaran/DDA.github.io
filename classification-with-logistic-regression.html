<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 4 Classification with Logistic Regression | Module Notes</title>
  <meta name="description" content="Module notes for Data Driven Analytics." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 4 Classification with Logistic Regression | Module Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Module notes for Data Driven Analytics." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 4 Classification with Logistic Regression | Module Notes" />
  
  <meta name="twitter:description" content="Module notes for Data Driven Analytics." />
  

<meta name="author" content="Dylan Viswambaran" />


<meta name="date" content="2022-06-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-with-k-nearest-neighbours-knn.html"/>
<link rel="next" href="linear-polynomial-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Driven Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Driven Analytics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#data-structures-in-python"><i class="fa fa-check"></i><b>1.1</b> Data Structures in Python</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#structured-vs-unstructured-data"><i class="fa fa-check"></i><b>1.2</b> Structured vs Unstructured data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Driven Pipelines</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#example-of-a-data-driven-pipeline-using-the-iris-dataset"><i class="fa fa-check"></i>Example of a data driven pipeline using the iris dataset</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-acquisition"><i class="fa fa-check"></i>Data acquisition</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning"><i class="fa fa-check"></i>Data cleaning</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-exploration"><i class="fa fa-check"></i>Data exploration</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-modelling"><i class="fa fa-check"></i>Data modelling</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-visualisation"><i class="fa fa-check"></i>Data visualisation</a></li>
</ul></li>
<li class="chapter" data-level="2.1" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning-1"><i class="fa fa-check"></i><b>2.1</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#how-do-we-clean-data"><i class="fa fa-check"></i>How do we clean data?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours (KNN)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#key-information"><i class="fa fa-check"></i><b>3.1</b> Key information</a></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages-and-disadvantages-of-using-the-knn-model"><i class="fa fa-check"></i><b>3.2</b> Advantages and Disadvantages of using the KNN model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages"><i class="fa fa-check"></i><b>3.2.1</b> Advantages</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#disadvantages"><i class="fa fa-check"></i><b>3.2.2</b> Disadvantages</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#things-to-guide-you-as-you-choose-the-value-of-k"><i class="fa fa-check"></i><b>3.2.3</b> Things to guide you as you choose the value of K</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#alternate-distance-metrics-to-euclidean"><i class="fa fa-check"></i><b>3.2.4</b> Alternate distance metrics to Euclidean</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#knn-code-example"><i class="fa fa-check"></i><b>3.2.5</b> KNN code example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Classification with Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#logistic-regression-assumptions"><i class="fa fa-check"></i><b>4.0.1</b> Logistic regression assumptions</a></li>
<li class="chapter" data-level="4.0.2" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#advantages-and-disadvantages-of-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.2</b> Advantages and Disadvantages of using Logistic Regression</a></li>
<li class="chapter" data-level="4.0.3" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#code-example-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.3</b> Code example using Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Linear &amp; Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#linear-regression"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#assumptions"><i class="fa fa-check"></i><b>5.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.3" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#classification-vs-regression"><i class="fa fa-check"></i><b>5.3</b> Classification vs Regression</a></li>
<li class="chapter" data-level="5.4" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-of-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Code example of Linear Regression</a></li>
<li class="chapter" data-level="5.5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-with-polynomial-regression"><i class="fa fa-check"></i><b>5.6</b> Code example with Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html"><i class="fa fa-check"></i><b>6</b> Probabilistic Classifiers and Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#naive-bayes-classification"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes Classification</a></li>
<li class="chapter" data-level="6.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-naive-bayes-classifier"><i class="fa fa-check"></i><b>6.2</b> Types of Naive Bayes Classifier</a></li>
<li class="chapter" data-level="6.3" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-and-disadvantages-of-naive-bayes-classifers"><i class="fa fa-check"></i><b>6.3</b> Advantages and Disadvantages of Naive Bayes Classifers</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-2"><i class="fa fa-check"></i><b>6.3.1</b> Advantages</a></li>
<li class="chapter" data-level="6.3.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-2"><i class="fa fa-check"></i><b>6.3.2</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#multinomial-naive-bayes"><i class="fa fa-check"></i><b>6.4</b> Multinomial Naive Bayes</a></li>
<li class="chapter" data-level="6.5" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#gradient-descent"><i class="fa fa-check"></i><b>6.5</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-gradient-descent"><i class="fa fa-check"></i><b>6.5.1</b> Types of Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html"><i class="fa fa-check"></i><b>7</b> Model Evaluation for Regression and Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>7.1</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="7.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#issues-with-train-test-split"><i class="fa fa-check"></i><b>7.2.1</b> Issues with train-test-split</a></li>
<li class="chapter" data-level="7.2.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#cross-validation"><i class="fa fa-check"></i><b>7.2.2</b> Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#measuring-model-performance"><i class="fa fa-check"></i><b>7.3</b> Measuring Model Performance</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#variance-and-sensitivity"><i class="fa fa-check"></i><b>7.3.1</b> Variance and sensitivity</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>7.3.2</b> Confusion Matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html"><i class="fa fa-check"></i><b>8</b> Advanced Statistical Learning Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#unsupervised-learning"><i class="fa fa-check"></i><b>8.1</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#clustering"><i class="fa fa-check"></i><b>8.1.1</b> Clustering</a></li>
<li class="chapter" data-level="8.1.2" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#decision-trees"><i class="fa fa-check"></i><b>8.1.2</b> Decision Trees</a></li>
<li class="chapter" data-level="8.1.3" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#principal-component-analysis"><i class="fa fa-check"></i><b>8.1.3</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="8.1.4" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#support-vector-machine"><i class="fa fa-check"></i><b>8.1.4</b> Support Vector Machine</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-database-systems.html"><a href="relational-database-systems.html"><i class="fa fa-check"></i><b>9</b> Relational Database Systems</a>
<ul>
<li class="chapter" data-level="9.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#sqlite"><i class="fa fa-check"></i><b>9.1</b> SQLite</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>9.1.1</b> Advantages and Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Module Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-with-logistic-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Module 4</span> Classification with Logistic Regression<a href="classification-with-logistic-regression.html#classification-with-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<blockquote>
<p>Logistic regression is a classification algorithm. It is used to predict a binary/ multinomial outcome based on a set of independent variables. The logistic regression model uses the Sigmoid function that is based on an estimated probability.</p>
</blockquote>
<div id="logistic-regression-assumptions" class="section level3 hasAnchor" number="4.0.1">
<h3><span class="header-section-number">4.0.1</span> Logistic regression assumptions<a href="classification-with-logistic-regression.html#logistic-regression-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>The dependent variable is binary or dichotomous—</strong>i.e. It fits into one of two clear-cut categories. This applies to binary logistic regression, which is the type of logistic regression we’ve discussed so far. We’ll explore some other types of logistic regression in section five.<br />
</li>
<li><strong>There should be no, or very little, multicollinearity between the predictor variables—</strong>in other words, the predictor variables (or the independent variables) should be independent of each other. This means that there should not be a high correlation between the independent variables.<br />
</li>
<li><strong>The independent variables should be linearly related to the log odds.</strong><br />
</li>
<li><strong>Logistic regression requires fairly large sample sizes—</strong>the larger the sample size, the more reliable (and powerful) you can expect the results of your analysis to be.</li>
</ul>
</div>
<div id="advantages-and-disadvantages-of-using-logistic-regression" class="section level3 hasAnchor" number="4.0.2">
<h3><span class="header-section-number">4.0.2</span> Advantages and Disadvantages of using Logistic Regression<a href="classification-with-logistic-regression.html#advantages-and-disadvantages-of-using-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="advantages-1" class="section level4 hasAnchor" number="4.0.2.1">
<h4><span class="header-section-number">4.0.2.1</span> Advantages<a href="classification-with-logistic-regression.html#advantages-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Logistic regression is much easier to implement than other methods, especially in the context of machine learning:</strong> Logistic regression is easier to train and implement as compared to other methods.</p></li>
<li><p><strong>Logistic regression works well for cases where the dataset is linearly separable:</strong> A dataset is said to be linearly separable if it is possible to draw a straight line that can separate the two classes of data from each other. Logistic regression is used when your Y variable can take only two values, and if the data is linearly separable, it is more efficient to classify it into two separate classes.</p></li>
<li><p><strong>Logistic regression provides useful insights:</strong> Logistic regression not only gives a measure of how relevant an independent variable is (i.e. the (coefficient size), but also tells us about the direction of the relationship (positive or negative). Two variables are said to have a positive association when an increase in the value of one variable also increases the value of the other variable.</p></li>
</ul>
</div>
<div id="disadvantages-1" class="section level4 hasAnchor" number="4.0.2.2">
<h4><span class="header-section-number">4.0.2.2</span> Disadvantages<a href="classification-with-logistic-regression.html#disadvantages-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Logistic regression fails to predict a continuous outcome.</strong></p></li>
<li><p><strong>Logistic regression assumes linearity between the predicted (dependent) variable and the predictor (independent) variables.</strong> Why is this a limitation? In the real world, it is highly unlikely that the observations are linearly separable. Let’s imagine you want to classify the iris plant into one of two families: sentosa or versicolor. In order to distinguish between the two categories, you’re going by petal size and sepal size. You want to create an algorithm to classify the iris plant, but there’s actually no clear distinction—a petal size of 2cm could qualify the plant for both the sentosa and versicolor categories. So, while linearly separable data is the assumption for logistic regression, in reality, it’s not always truly possible.</p></li>
<li><p><strong>Logistic regression may not be accurate if the sample size is too small.</strong> If the sample size is on the small side, the model produced by logistic regression is based on a smaller number of actual observations. This can result in overfitting. In statistics, overfitting is a modeling error which occurs when the model is too closely fit to a limited set of data because of a lack of training data. Or, in other words, there is not enough input data available for the model to find patterns in it. In this case, the model is not able to accurately predict the outcomes of a new or future dataset.</p></li>
</ul>
</div>
</div>
<div id="code-example-using-logistic-regression" class="section level3 hasAnchor" number="4.0.3">
<h3><span class="header-section-number">4.0.3</span> Code example using Logistic Regression<a href="classification-with-logistic-regression.html#code-example-using-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<pre><code># **PART 1: Using Logistic Regression in the Iris dataset to predict a flower&#39;s 
variety (Classification problem)**
**Question 1:** Load the *Iris.csv* file in the Colab and *read* it as a dataframe.
&quot;&quot;&quot;
# Provide your solution here
import pandas as pd
iris_data = pd.read_csv(&#39;Iris.csv&#39;)
iris_data
&quot;&quot;&quot;**Question 2:** Examine the data schema by printing the *types* of data for each
column. Provide the answer for each question.&quot;&quot;&quot;
# Provide your solution here
iris_data.dtypes
# How many columns do we have? __5__
# What type is the sepal.length column? __String__
&quot;&quot;&quot;**Question 3:** Continue examining the dataset, let us focus on the numerical 
data, columns include:
*  sepal.length -&gt; float64
*  sepal.width -&gt; float64
*  petal.length -&gt; float64
*  petal.width -&gt; float64
Describe the dataset and answer the following questions
&quot;&quot;&quot;
# Provide your solution here
iris_data.describe()
&quot;&quot;&quot;**Question 4:** We will need to split our dataset into feature data **(X)** and 
target data **(y)**.
We will use the sepal length and width, and the petal length and width to predict a
flower&#39;s class (variety).
So, the feature data is:
*  sepal.length
*  sepal.width
*  petal.length
*  petal.width
The target data is:
*  variety
Extract the data for the feature variables (and associated columns) and store it in
a new dataframe called **iris_data_feature_data**. This will be our **X** variable.
&quot;&quot;&quot;
# Provide your solution here
iris_data_feature_data = 
iris_data[[&#39;sepal.length&#39;,&#39;sepal.width&#39;,&#39;petal.length&#39;,&#39;petal.width&#39;]]
X = iris_data_feature_data
&quot;&quot;&quot;**Question 5:** Now we will need to create the **iris_data_feature_target** 
dataframe. The new dataframe will store only the target values that is the variety 
column. This will be our **y** variable.&quot;&quot;&quot;
# Provide your solution here
iris_data_feature_target = iris_data[[&#39;variety&#39;]]
y = iris_data_feature_target
&quot;&quot;&quot;**Question 6:** Create a new *LogisticRegression model* and then fit the **X** 
and **y** dataframes that we just created. Make sure you revise the class slides to
create your model.
**Do not worry about the warnings.**
&quot;&quot;&quot;
# Provide your solution here
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(max_iter=200)
logreg.fit(X,y)
&quot;&quot;&quot;**Question 7:** Our model is now ready to use, test your model using the 
following data to predict a flower&#39;s class:
**Set 1**
*  sepal.length: 4.5
*  sepal.width: 3.2
*  petal.length: 1.1
*  petal.width: 0.4
**Set 2**
*  sepal.length: 6.9
*  sepal.width: 3.0
*  petal.length: 4.6
*  petal.width: 1.5
**Set 3**
*  sepal.length: 6
*  sepal.width: 3.5
*  petal.length: 5.5
*  petal.width: 2
&quot;&quot;&quot;
# Provide your solution here
# Prediction for set 1
print(logreg.predict([[4.5, 3.2, 1.1, 0.4]]))
# Prediction for set 2
print(logreg.predict([[6.9, 3.0, 4.6, 1.5]]))
# Prediction for set 3
print(logreg.predict([[6, 3.5, 5.5, 2]]))
&quot;&quot;&quot;**Question 8:** What is the prediction of the KNN model for the same data sets? 
Revise last week&#39;s exercises if needed.
Create your KNN model and fit it using the **X** and **y** variables from the 
previous step. Use the *KNeighborsClassifier* including 4 neighbors.
&quot;&quot;&quot;
# Provide your solution here
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X, y)
&quot;&quot;&quot;**Question 9:** Our KNN model is now ready to use, test your model using the 
following data to predict a flower&#39;s class (same as in question 7):
**Set 1**
*  sepal.length: 4.5
*  sepal.width: 3.2
*  petal.length: 1.1
*  petal.width: 0.4
**Set 2**
*  sepal.length: 6.9
*  sepal.width: 3.0
*  petal.length: 4.6
*  petal.width: 1.5
**Set 3**
*  sepal.length: 6
*  sepal.width: 3.5
*  petal.length: 5.5
*  petal.width: 2
&quot;&quot;&quot;
# Provide your solution here
# Prediction for set 1
print(knn.predict([[4.5, 3.2, 1.1, 0.4]]))
# Prediction for set 2
print(knn.predict([[6.9, 3.0, 4.6, 1.5]]))
# Prediction for set 3
print(knn.predict([[6, 3.5, 5.5, 2]]))
&quot;&quot;&quot;**Question 10:** Compare the results between logistic regression and KNN.&quot;&quot;&quot;
# We have the same or different predictions?
# __Same__!</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-with-k-nearest-neighbours-knn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-polynomial-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/04-Module4.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
