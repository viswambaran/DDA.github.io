<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 6 Probabilistic Classifiers and Gradient Descent | Module Notes</title>
  <meta name="description" content="Module notes for Data Driven Analytics." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 6 Probabilistic Classifiers and Gradient Descent | Module Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Module notes for Data Driven Analytics." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 6 Probabilistic Classifiers and Gradient Descent | Module Notes" />
  
  <meta name="twitter:description" content="Module notes for Data Driven Analytics." />
  

<meta name="author" content="Dylan Viswambaran" />


<meta name="date" content="2022-06-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-polynomial-regression.html"/>
<link rel="next" href="model-evaluation-for-regression-and-classification.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Driven Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Driven Analytics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#data-structures-in-python"><i class="fa fa-check"></i><b>1.1</b> Data Structures in Python</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#structured-vs-unstructured-data"><i class="fa fa-check"></i><b>1.2</b> Structured vs Unstructured data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Driven Pipelines</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#example-of-a-data-driven-pipeline-using-the-iris-dataset"><i class="fa fa-check"></i>Example of a data driven pipeline using the iris dataset</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-acquisition"><i class="fa fa-check"></i>Data acquisition</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning"><i class="fa fa-check"></i>Data cleaning</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-exploration"><i class="fa fa-check"></i>Data exploration</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-modelling"><i class="fa fa-check"></i>Data modelling</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-visualisation"><i class="fa fa-check"></i>Data visualisation</a></li>
</ul></li>
<li class="chapter" data-level="2.1" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning-1"><i class="fa fa-check"></i><b>2.1</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#how-do-we-clean-data"><i class="fa fa-check"></i>How do we clean data?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours (KNN)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#key-information"><i class="fa fa-check"></i><b>3.1</b> Key information</a></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages-and-disadvantages-of-using-the-knn-model"><i class="fa fa-check"></i><b>3.2</b> Advantages and Disadvantages of using the KNN model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages"><i class="fa fa-check"></i><b>3.2.1</b> Advantages</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#disadvantages"><i class="fa fa-check"></i><b>3.2.2</b> Disadvantages</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#things-to-guide-you-as-you-choose-the-value-of-k"><i class="fa fa-check"></i><b>3.2.3</b> Things to guide you as you choose the value of K</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#alternate-distance-metrics-to-euclidean"><i class="fa fa-check"></i><b>3.2.4</b> Alternate distance metrics to Euclidean</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#knn-code-example"><i class="fa fa-check"></i><b>3.2.5</b> KNN code example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Classification with Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#logistic-regression-assumptions"><i class="fa fa-check"></i><b>4.0.1</b> Logistic regression assumptions</a></li>
<li class="chapter" data-level="4.0.2" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#advantages-and-disadvantages-of-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.2</b> Advantages and Disadvantages of using Logistic Regression</a></li>
<li class="chapter" data-level="4.0.3" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#code-example-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.3</b> Code example using Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Linear &amp; Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#linear-regression"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#assumptions"><i class="fa fa-check"></i><b>5.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.3" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#classification-vs-regression"><i class="fa fa-check"></i><b>5.3</b> Classification vs Regression</a></li>
<li class="chapter" data-level="5.4" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-of-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Code example of Linear Regression</a></li>
<li class="chapter" data-level="5.5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-with-polynomial-regression"><i class="fa fa-check"></i><b>5.6</b> Code example with Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html"><i class="fa fa-check"></i><b>6</b> Probabilistic Classifiers and Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#naive-bayes-classification"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes Classification</a></li>
<li class="chapter" data-level="6.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-naive-bayes-classifier"><i class="fa fa-check"></i><b>6.2</b> Types of Naive Bayes Classifier</a></li>
<li class="chapter" data-level="6.3" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-and-disadvantages-of-naive-bayes-classifers"><i class="fa fa-check"></i><b>6.3</b> Advantages and Disadvantages of Naive Bayes Classifers</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-2"><i class="fa fa-check"></i><b>6.3.1</b> Advantages</a></li>
<li class="chapter" data-level="6.3.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-2"><i class="fa fa-check"></i><b>6.3.2</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#multinomial-naive-bayes"><i class="fa fa-check"></i><b>6.4</b> Multinomial Naive Bayes</a></li>
<li class="chapter" data-level="6.5" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#gradient-descent"><i class="fa fa-check"></i><b>6.5</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-gradient-descent"><i class="fa fa-check"></i><b>6.5.1</b> Types of Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html"><i class="fa fa-check"></i><b>7</b> Model Evaluation for Regression and Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>7.1</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="7.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#issues-with-train-test-split"><i class="fa fa-check"></i><b>7.2.1</b> Issues with train-test-split</a></li>
<li class="chapter" data-level="7.2.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#cross-validation"><i class="fa fa-check"></i><b>7.2.2</b> Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#measuring-model-performance"><i class="fa fa-check"></i><b>7.3</b> Measuring Model Performance</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#variance-and-sensitivity"><i class="fa fa-check"></i><b>7.3.1</b> Variance and sensitivity</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>7.3.2</b> Confusion Matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html"><i class="fa fa-check"></i><b>8</b> Advanced Statistical Learning Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#unsupervised-learning"><i class="fa fa-check"></i><b>8.1</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#clustering"><i class="fa fa-check"></i><b>8.1.1</b> Clustering</a></li>
<li class="chapter" data-level="8.1.2" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#decision-trees"><i class="fa fa-check"></i><b>8.1.2</b> Decision Trees</a></li>
<li class="chapter" data-level="8.1.3" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#principal-component-analysis"><i class="fa fa-check"></i><b>8.1.3</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="8.1.4" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#support-vector-machine"><i class="fa fa-check"></i><b>8.1.4</b> Support Vector Machine</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-database-systems.html"><a href="relational-database-systems.html"><i class="fa fa-check"></i><b>9</b> Relational Database Systems</a>
<ul>
<li class="chapter" data-level="9.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#sqlite"><i class="fa fa-check"></i><b>9.1</b> SQLite</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>9.1.1</b> Advantages and Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Module Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probabilistic-classifiers-and-gradient-descent" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Module 6</span> Probabilistic Classifiers and Gradient Descent<a href="probabilistic-classifiers-and-gradient-descent.html#probabilistic-classifiers-and-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="naive-bayes-classification" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Naive Bayes Classification<a href="probabilistic-classifiers-and-gradient-descent.html#naive-bayes-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.</p>
</blockquote>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
\]</span></p>
<p>** Learning Rate **</p>
<blockquote>
<p>The size of these steps is called the learning rate. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.</p>
</blockquote>
<p>** Cost Function **</p>
<blockquote>
<p>A Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.</p>
</blockquote>
</div>
<div id="types-of-naive-bayes-classifier" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Types of Naive Bayes Classifier<a href="probabilistic-classifiers-and-gradient-descent.html#types-of-naive-bayes-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Multinomial Naive Bayes:</strong></p>
<p>This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.</p>
<p><strong>Bernoulli Naive Bayes:</strong></p>
<p>This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.</p>
<p><strong>Gaussian Naive Bayes:</strong><br />
When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.</p>
</div>
<div id="advantages-and-disadvantages-of-naive-bayes-classifers" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Advantages and Disadvantages of Naive Bayes Classifers<a href="probabilistic-classifiers-and-gradient-descent.html#advantages-and-disadvantages-of-naive-bayes-classifers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="advantages-2" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Advantages<a href="probabilistic-classifiers-and-gradient-descent.html#advantages-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Simple to Implement.</strong> The conditional probabilities are easy to evaluate.<br />
</li>
<li><strong>Very fast</strong> – no iterations since the probabilities can be directly computed. So this technique is useful where speed of training is important.<br />
</li>
<li><strong>Good classification results</strong> - If the conditional Independence assumption holds, it could give great results.</li>
</ul>
</div>
<div id="disadvantages-2" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Disadvantages<a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Assumptions for class conditionals are independence<br />
</li>
<li>Practically, dependencies exist among variables which cannot be modelled by Naive Bayes. e.g. patient profile with symptoms.</li>
</ul>
</div>
</div>
<div id="multinomial-naive-bayes" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Multinomial Naive Bayes<a href="probabilistic-classifiers-and-gradient-descent.html#multinomial-naive-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>Multinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output.</p>
</blockquote>
</div>
<div id="gradient-descent" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Gradient Descent<a href="probabilistic-classifiers-and-gradient-descent.html#gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>Gradient descent is an optimization algorithm that’s used when training a machine learning model. It’s based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.</p>
</blockquote>
<div id="types-of-gradient-descent" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Types of Gradient Descent<a href="probabilistic-classifiers-and-gradient-descent.html#types-of-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="batch-gradient-descent" class="section level4 hasAnchor" number="6.5.1.1">
<h4><span class="header-section-number">6.5.1.1</span> Batch Gradient Descent<a href="probabilistic-classifiers-and-gradient-descent.html#batch-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Batch gradient descent sums the error for each point in a training set, updating the model only after all training examples have been evaluated. This process referred to as a training epoch.</p>
<p>While this batching provides computation efficiency, it can still have a long processing time for large training datasets as it still needs to store all of the data into memory. Batch gradient descent also usually produces a stable error gradient and convergence, but sometimes that convergence point isn’t the most ideal, finding the local minimum versus the global one.</p>
<div id="advantages-3" class="section level5 hasAnchor" number="6.5.1.1.1">
<h5><span class="header-section-number">6.5.1.1.1</span> Advantages<a href="probabilistic-classifiers-and-gradient-descent.html#advantages-3" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>More stable convergence and error gradient than Stochastic Gradient descent<br />
</li>
<li>Embraces the benefits of vectorization<br />
</li>
<li>A more direct path is taken towards the minimum<br />
</li>
<li>Computationally efficient since updates are required after the run of an epoch</li>
</ul>
</div>
<div id="disadvantages-3" class="section level5 hasAnchor" number="6.5.1.1.2">
<h5><span class="header-section-number">6.5.1.1.2</span> Disadvantages<a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-3" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Can converge at local minima and saddle points<br />
</li>
<li>Slower learning since an update is performed only after we go through all observations</li>
</ul>
</div>
</div>
<div id="stochastic-gradient-descent" class="section level4 hasAnchor" number="6.5.1.2">
<h4><span class="header-section-number">6.5.1.2</span> Stochastic Gradient Descent<a href="probabilistic-classifiers-and-gradient-descent.html#stochastic-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Stochastic gradient descent (SGD) runs a training epoch for each example within the dataset and it updates each training example’s parameters one at a time. Since you only need to hold one training example, they are easier to store in memory. While these frequent updates can offer more detail and speed, it can result in losses in computational efficiency when compared to batch gradient descent. Its frequent updates can result in noisy gradients, but this can also be helpful in escaping the local minimum and finding the global one.</p>
<div id="advantages-4" class="section level5 hasAnchor" number="6.5.1.2.1">
<h5><span class="header-section-number">6.5.1.2.1</span> Advantages<a href="probabilistic-classifiers-and-gradient-descent.html#advantages-4" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Only a single observation is being processed by the network so it is easier to fit into memory<br />
</li>
<li>May (likely) to reach near the minimum (and begin to oscillate) faster than Batch Gradient Descent on a large dataset<br />
</li>
<li>The frequent updates create plenty of oscillations which can be helpful for getting out of local minimums.</li>
</ul>
</div>
<div id="disadvantages-4" class="section level5 hasAnchor" number="6.5.1.2.2">
<h5><span class="header-section-number">6.5.1.2.2</span> Disadvantages<a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-4" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Can veer off in the wrong direction due to frequent updates<br />
</li>
<li>Lose the benefits of vectorization since we process one observation per time<br />
</li>
<li>Frequent updates are computationally expensive due to using all resources for processing one training sample at a time</li>
</ul>
</div>
</div>
<div id="mini-batch-gradient-descent" class="section level4 hasAnchor" number="6.5.1.3">
<h4><span class="header-section-number">6.5.1.3</span> Mini-batch Gradient Descent<a href="probabilistic-classifiers-and-gradient-descent.html#mini-batch-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Mini-batch gradient descent combines concepts from both batch gradient descent and stochastic gradient descent. It splits the training dataset into small batch sizes and performs updates on each of those batches. This approach strikes a balance between the computational efficiency of batch gradient descent and the speed of stochastic gradient descent.</p>
<div id="advantages-5" class="section level5 hasAnchor" number="6.5.1.3.1">
<h5><span class="header-section-number">6.5.1.3.1</span> Advantages<a href="probabilistic-classifiers-and-gradient-descent.html#advantages-5" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Convergence is more stable than Stochastic Gradient Descent<br />
</li>
<li>Computationally efficient<br />
</li>
<li>Fast Learning since we perform more updates</li>
</ul>
</div>
<div id="disadvantages-5" class="section level5 hasAnchor" number="6.5.1.3.2">
<h5><span class="header-section-number">6.5.1.3.2</span> Disadvantages<a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-5" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>We have to configure the Mini-Batch size hyperparameter</li>
</ul>

</div>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-polynomial-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-evaluation-for-regression-and-classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/06-Module6.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
