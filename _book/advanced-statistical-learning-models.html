<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 8 Advanced Statistical Learning Models | Module Notes</title>
  <meta name="description" content="Module notes for Data Driven Analytics." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 8 Advanced Statistical Learning Models | Module Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Module notes for Data Driven Analytics." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 8 Advanced Statistical Learning Models | Module Notes" />
  
  <meta name="twitter:description" content="Module notes for Data Driven Analytics." />
  

<meta name="author" content="Dylan Viswambaran" />


<meta name="date" content="2022-06-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="model-evaluation-for-regression-and-classification.html"/>
<link rel="next" href="relational-database-systems.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Driven Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Driven Analytics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#data-structures-in-python"><i class="fa fa-check"></i><b>1.1</b> Data Structures in Python</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-data-driven-analytics.html"><a href="introduction-to-data-driven-analytics.html#structured-vs-unstructured-data"><i class="fa fa-check"></i><b>1.2</b> Structured vs Unstructured data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Driven Pipelines</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#example-of-a-data-driven-pipeline-using-the-iris-dataset"><i class="fa fa-check"></i>Example of a data driven pipeline using the iris dataset</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-acquisition"><i class="fa fa-check"></i>Data acquisition</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning"><i class="fa fa-check"></i>Data cleaning</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-exploration"><i class="fa fa-check"></i>Data exploration</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-modelling"><i class="fa fa-check"></i>Data modelling</a></li>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-visualisation"><i class="fa fa-check"></i>Data visualisation</a></li>
</ul></li>
<li class="chapter" data-level="2.1" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#data-cleaning-1"><i class="fa fa-check"></i><b>2.1</b> Data cleaning</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-data-driven-pipelines.html"><a href="introduction-to-data-driven-pipelines.html#how-do-we-clean-data"><i class="fa fa-check"></i>How do we clean data?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours (KNN)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#key-information"><i class="fa fa-check"></i><b>3.1</b> Key information</a></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages-and-disadvantages-of-using-the-knn-model"><i class="fa fa-check"></i><b>3.2</b> Advantages and Disadvantages of using the KNN model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#advantages"><i class="fa fa-check"></i><b>3.2.1</b> Advantages</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#disadvantages"><i class="fa fa-check"></i><b>3.2.2</b> Disadvantages</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#things-to-guide-you-as-you-choose-the-value-of-k"><i class="fa fa-check"></i><b>3.2.3</b> Things to guide you as you choose the value of K</a></li>
<li class="chapter" data-level="3.2.4" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#alternate-distance-metrics-to-euclidean"><i class="fa fa-check"></i><b>3.2.4</b> Alternate distance metrics to Euclidean</a></li>
<li class="chapter" data-level="3.2.5" data-path="classification-with-k-nearest-neighbours-knn.html"><a href="classification-with-k-nearest-neighbours-knn.html#knn-code-example"><i class="fa fa-check"></i><b>3.2.5</b> KNN code example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Classification with Logistic Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#logistic-regression-assumptions"><i class="fa fa-check"></i><b>4.0.1</b> Logistic regression assumptions</a></li>
<li class="chapter" data-level="4.0.2" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#advantages-and-disadvantages-of-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.2</b> Advantages and Disadvantages of using Logistic Regression</a></li>
<li class="chapter" data-level="4.0.3" data-path="classification-with-logistic-regression.html"><a href="classification-with-logistic-regression.html#code-example-using-logistic-regression"><i class="fa fa-check"></i><b>4.0.3</b> Code example using Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html"><i class="fa fa-check"></i><b>5</b> Linear &amp; Polynomial Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#linear-regression"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#assumptions"><i class="fa fa-check"></i><b>5.2</b> Assumptions</a></li>
<li class="chapter" data-level="5.3" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#classification-vs-regression"><i class="fa fa-check"></i><b>5.3</b> Classification vs Regression</a></li>
<li class="chapter" data-level="5.4" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-of-linear-regression"><i class="fa fa-check"></i><b>5.4</b> Code example of Linear Regression</a></li>
<li class="chapter" data-level="5.5" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>5.5</b> Polynomial Regression</a></li>
<li class="chapter" data-level="5.6" data-path="linear-polynomial-regression.html"><a href="linear-polynomial-regression.html#code-example-with-polynomial-regression"><i class="fa fa-check"></i><b>5.6</b> Code example with Polynomial regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html"><i class="fa fa-check"></i><b>6</b> Probabilistic Classifiers and Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#naive-bayes-classification"><i class="fa fa-check"></i><b>6.1</b> Naive Bayes Classification</a></li>
<li class="chapter" data-level="6.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-naive-bayes-classifier"><i class="fa fa-check"></i><b>6.2</b> Types of Naive Bayes Classifier</a></li>
<li class="chapter" data-level="6.3" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-and-disadvantages-of-naive-bayes-classifers"><i class="fa fa-check"></i><b>6.3</b> Advantages and Disadvantages of Naive Bayes Classifers</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#advantages-2"><i class="fa fa-check"></i><b>6.3.1</b> Advantages</a></li>
<li class="chapter" data-level="6.3.2" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#disadvantages-2"><i class="fa fa-check"></i><b>6.3.2</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#multinomial-naive-bayes"><i class="fa fa-check"></i><b>6.4</b> Multinomial Naive Bayes</a></li>
<li class="chapter" data-level="6.5" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#gradient-descent"><i class="fa fa-check"></i><b>6.5</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="probabilistic-classifiers-and-gradient-descent.html"><a href="probabilistic-classifiers-and-gradient-descent.html#types-of-gradient-descent"><i class="fa fa-check"></i><b>6.5.1</b> Types of Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html"><i class="fa fa-check"></i><b>7</b> Model Evaluation for Regression and Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#what-is-model-evaluation"><i class="fa fa-check"></i><b>7.1</b> What is Model Evaluation?</a></li>
<li class="chapter" data-level="7.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#methods"><i class="fa fa-check"></i><b>7.2</b> Methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#issues-with-train-test-split"><i class="fa fa-check"></i><b>7.2.1</b> Issues with train-test-split</a></li>
<li class="chapter" data-level="7.2.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#cross-validation"><i class="fa fa-check"></i><b>7.2.2</b> Cross validation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#measuring-model-performance"><i class="fa fa-check"></i><b>7.3</b> Measuring Model Performance</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#variance-and-sensitivity"><i class="fa fa-check"></i><b>7.3.1</b> Variance and sensitivity</a></li>
<li class="chapter" data-level="7.3.2" data-path="model-evaluation-for-regression-and-classification.html"><a href="model-evaluation-for-regression-and-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>7.3.2</b> Confusion Matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html"><i class="fa fa-check"></i><b>8</b> Advanced Statistical Learning Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#unsupervised-learning"><i class="fa fa-check"></i><b>8.1</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#clustering"><i class="fa fa-check"></i><b>8.1.1</b> Clustering</a></li>
<li class="chapter" data-level="8.1.2" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#decision-trees"><i class="fa fa-check"></i><b>8.1.2</b> Decision Trees</a></li>
<li class="chapter" data-level="8.1.3" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#principal-component-analysis"><i class="fa fa-check"></i><b>8.1.3</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="8.1.4" data-path="advanced-statistical-learning-models.html"><a href="advanced-statistical-learning-models.html#support-vector-machine"><i class="fa fa-check"></i><b>8.1.4</b> Support Vector Machine</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="relational-database-systems.html"><a href="relational-database-systems.html"><i class="fa fa-check"></i><b>9</b> Relational Database Systems</a>
<ul>
<li class="chapter" data-level="9.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#sqlite"><i class="fa fa-check"></i><b>9.1</b> SQLite</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="relational-database-systems.html"><a href="relational-database-systems.html#advantages-and-disadvantages-1"><i class="fa fa-check"></i><b>9.1.1</b> Advantages and Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Module Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="advanced-statistical-learning-models" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Module 8</span> Advanced Statistical Learning Models<a href="advanced-statistical-learning-models.html#advanced-statistical-learning-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="unsupervised-learning" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Unsupervised learning<a href="advanced-statistical-learning-models.html#unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervention. Its ability to discover similarities and differences in information make it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition.</p>
</blockquote>
<ul>
<li><p>The data has no target attribute</p></li>
<li><p>Explore the data to find intrinsic structures in them.</p>
<ul>
<li>Grouping similar instances together.<br />
</li>
</ul></li>
<li><p>Draws inferences from datasets without labels<br />
</p></li>
<li><p>Best if you arent sure what the target variable should be.</p></li>
</ul>
<p>We can use <code>Clustering</code> to group items together based on similiarity.</p>
<div id="clustering" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Clustering<a href="advanced-statistical-learning-models.html#clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Clustering is a data mining technique that groups unlabeled data based on their similarities or differences. Clustering algorithms are used to process raw, unclassified data objects into groups represented by structures or patterns in the information. Clustering algorithms can be categorized into a few types: exclusive, overlapping, hierarchical, and probabilistic.</p>
</blockquote>
<div id="k-means-clustering" class="section level4 hasAnchor" number="8.1.1.1">
<h4><span class="header-section-number">8.1.1.1</span> K-Means Clustering<a href="advanced-statistical-learning-models.html#k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>K-Means is a <strong>partitional clustering algorithm</strong></li>
<li>Clustering is a technique for finding similarity groups in data called clusters.
<ul>
<li>Groups data instances that are similar to each other in one cluster and data instances that are very different from each other into different clusters.</li>
</ul></li>
<li>K-Means is very efficient at clustering</li>
<li>Tries to find the centre of each cluster and assign each instance to the closest cluster.</li>
</ul>
<div id="how-does-it-work" class="section level5 hasAnchor" number="8.1.1.1.1">
<h5><span class="header-section-number">8.1.1.1.1</span> How does it work?<a href="advanced-statistical-learning-models.html#how-does-it-work" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Randomly choose k data points to be the initial centroids, cluster centres.<br />
</li>
<li>Assign each data point to the closest centroid<br />
</li>
<li>Re-compute the centroids using the current cluster memberships<br />
</li>
<li>If a convergence criterion is not met repeat step 2.</li>
</ul>
<p>When does the model stop?</p>
<ul>
<li>No or min re-assignments of data points to the different clusters.<br />
</li>
<li>No or min change of centroids<br />
</li>
<li>Minimum decrease in the sum of squared error (SSE)</li>
</ul>
</div>
<div id="advantages-and-disadvantages" class="section level5 hasAnchor" number="8.1.1.1.2">
<h5><span class="header-section-number">8.1.1.1.2</span> Advantages and Disadvantages<a href="advanced-statistical-learning-models.html#advantages-and-disadvantages" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div id="advantages-6" class="section level6 hasAnchor" number="8.1.1.1.2.1">
<h6><span class="header-section-number">8.1.1.1.2.1</span> Advantages<a href="advanced-statistical-learning-models.html#advantages-6" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<ul>
<li>It is simple, highly flexible, and efficient. The simplicity of k-means makes it easy to explain the results in contrast to Neural Networks.<br />
</li>
<li>The flexibility of k-means allows for easy adjustment if there are problems.<br />
</li>
<li>The efficiency of k-means implies that the algorithm is good at segmenting a dataset.<br />
</li>
<li>An instance can change cluster (move to another cluster) when the centroids are recomputed<br />
</li>
<li>Easy to interpret the clustering results.</li>
</ul>
</div>
<div id="disadvantages-6" class="section level6 hasAnchor" number="8.1.1.1.2.2">
<h6><span class="header-section-number">8.1.1.1.2.2</span> Disadvantages<a href="advanced-statistical-learning-models.html#disadvantages-6" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<ul>
<li><p>It does not allow to develop the most optimal set of clusters and the number of clusters must be decided before the analysis. How many clusters to include is left at the discretion of the researcher. This involves a combination of common sense, domain knowledge, and statistical tools. Too many clusters tell you nothing because of the groups becoming very small and there are too many of them. There are statistical tools that measure within-group homogeneity and group heterogeneity. There are methods like the elbow method to decide the value of k. Additionally, there is a technique called a dendrogram. The results of a dendrogram analysis provide a recommendation of how many clusters to use. However, calculating a dendrogram for a large dataset could potentially crash a computer due to the computational load and the limits of RAM.</p></li>
<li><p>When doing the analysis, the k-means algorithm will randomly select several different places from which to develop clusters. This can be good or bad depending on where the algorithm chooses to begin at. From there, the centre of the clusters is recalculated until an adequate “centre’’ is found for the number of clusters requested.</p></li>
<li><p>The order of the data has an impact on the final results.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="decision-trees" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Decision Trees<a href="advanced-statistical-learning-models.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>A decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails). Each leaf/node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from the root to the leaf represent classification rules.</p>
</blockquote>
</div>
<div id="principal-component-analysis" class="section level3 hasAnchor" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Principal Component Analysis<a href="advanced-statistical-learning-models.html#principal-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.</p>
</blockquote>
</div>
<div id="support-vector-machine" class="section level3 hasAnchor" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> Support Vector Machine<a href="advanced-statistical-learning-models.html#support-vector-machine" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning.<br />
The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.<br />
SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called support vectors, and hence the algorithm is termed a Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane</p>
</blockquote>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-evaluation-for-regression-and-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="relational-database-systems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/08-Module8.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
