[["introduction-to-data-driven-analytics.html", "Module Notes Module 1 Introduction to Data Driven Analytics 1.1 Data Structures in Python 1.2 Structured vs Unstructured data", " Module Notes Dylan Viswambaran 2022-06-16 Module 1 Introduction to Data Driven Analytics 1.1 Data Structures in Python Lists represent a sequence of items indexed by their integer position. Lists contain zero or more elements and can contain elements of different data types or data structures (even nested objects). This makes lists powerful because they allow you to create deep and complex data structures. Tuples are similar to lists but are immutable; you cannot add, delete, or change items after a tuple is created. Tuples differ from lists by having fewer functions since you cannot modify them. Tuples contain zero or more elements and can contain elements of different, immutable types. Dictionaries use keys to associate with each value. This means that we don’t keep track of the order of elements. Dictionary keys are immutable and unique, however, dictionaries are mutable; the key-value elements can be added, deleted, or changed. Dictionaries are very similar to hashmaps. Sets are like a dictionary but with only the keys, not the values. This means that sets are unique and not sequential (stored unordered). Sets are also mutable. Sets contain zero or more elements and can contain elements of different, immutable types. Dataframe is a fundamental data structure of the panda’s package but its use is so widespread that we mention it here. It organizes data into a 2-dimensional table of rows and columns, much like a spreadsheet. DataFrames are one of the most common data structures used in modern data analytics because they are a flexible and intuitive way of storing and working with data. 1.2 Structured vs Unstructured data Structured data is most often categorized as quantitative data, and it’s the type of data most of us are used to working with. Think of data that fits neatly within fixed fields and columns in relational databases. Examples of structured data include tables in SQL systems with users, customers and their orders. Structured data is highly organized, thus easily understood by machine language. Working with relational databases, you can input, search, and manipulate structured data relatively quickly by using a relational database management system (RDBMS). This is the most attractive feature of structured data. Typical characteristics include: Clearly defined data types A data model known as schema that define the data. e.g. Data type (integer, string, Boolean), data format (example@email.com) has defined constraints and rules numeric fields cant accept any string values, certain fields cant be left empty. data are stored within relational databases. Unstructured data, typically categorized as qualitative data, cannot be processed and analyzed via conventional data tools and methods. Since unstructured data does not have a predefined data model, it is best managed in non-relational (NoSQL) databases. The importance of unstructured data is rapidly increasing. Recent projections indicate that unstructured data is over 80% of all enterprise data, while 95% of businesses prioritize unstructured data management. Typical characteristics include: Internal structure is not defined. There is no data model (schema) Examples of unstructured data include PDFs, emails, tweets etc. data are stored in NoSQL databases. "],["introduction-to-data-driven-pipelines.html", "Module 2 Introduction to Data Driven Pipelines Example of a data driven pipeline using the iris dataset 2.1 Data cleaning", " Module 2 Introduction to Data Driven Pipelines A data pipeline is a means of transforming or moving data from one state to another, including data schemas and systems. Along the way, data is transformed and optimized, arriving in a state that can be analyzed and used to develop business insights. The steps of a Data Driven Pipeline include: Data acquisition Data cleaning Data exploration Data modeling Data visualisation Example of a data driven pipeline using the iris dataset Data acquisition Firstly, we would load the raw data (iris dataset) into our python script. The data format can be structured, un-structured or semi-structured. In our case, the data we are working with is structured. Data cleaning Examine the data (and clean if necessary) During the data cleaning stage, we would remove any incorrect, corrupted, incorrectly formatted, duplicated or incomplete data in a dataset. We will remove duplicated or irrelevant observations to minimise distraction from the primary target and to create a more performant dataset. Structural errors will be fixed by removing an N/A values, strange naming conventions or typos. Missing data will be dealt with by the process of transformation. We will also filter and unwanted outliers to help the performance of the data we are working with and to ensure the data is accepted by algorithms. At the end of the cleaning process, we will explore the data to validate that the results make sense. Data exploration Once the data is in the correct format, exploratory analysis will be performed to make sure it meets the project requirements. Exploratory techniques include data schema, feature analysis, checking for outliers and studying distribution. We will identify the target and feature variables. Distribution features such as a distribution curve, correlation matrix or boxplot can give a good indication of the structure of the dataset. This can help identify outliers and give an indication of the skew. If the data is skewed it will need to be transformed so it follows a normal distribution. It is vital that the variables are correlated, and non-linear features are transformed or else the model would not perform well. Data modelling Use the python library (scikit-learn) to create a KNN model using our dataset (the model will ‘learn’ based on our data sample). Make predictions for unknown data (predict the flower class). At this stage we can implement our regression, classification or clustering models. Data visualisation During the data visualisation stage we will use the visualisation packages matplotlib and seaborn. Benefits of a data driven pipeline are that it can analyse large amounts of data. They also reduce manual processes and allow for automation to occur when data flows from one state to another. They are helpful to make real-time, faster, data-driven decisions. 2.1 Data cleaning Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled. If data is incorrect, outcomes and algorithms are unreliable, even though they may look correct. There is no one absolute way to prescribe the exact steps in the data cleaning process because the processes will vary from dataset to dataset. But it is crucial to establish a template for your data cleaning process so you know you are doing it the right way every time. How do we clean data? We can follow the following steps as discussed in Data cleaning: The benefits and steps to creating and using clean data: Step 1: Remove duplicate or irrelevant observations: This can make analysis more efficient and minimize distraction from your primary target—as well as create a more manageable and more performant dataset. Step 2: Fix structural errors: Structural errors are when you measure or transfer data and notice strange naming conventions, typos, or incorrect capitalization. For example, you may find “N/A” and “Not Applicable” both appear, but they should be analyzed as the same category. Step 3: Filter unwanted outliers: Often, there will be one-off observations where, at a glance, they do not appear to fit within the data you are analyzing. If you have a legitimate reason to remove an outlier, like improper data-entry, doing so will help the performance of the data you are working with. Step 4: Handle missing data: You can’t ignore missing data because many algorithms will not accept missing values. There are a couple of ways to deal with missing data. Neither is optimal, but both can be considered. Step 5: Validate and QA: At the end of the data cleaning process, you should be able to explore the data to validate that the results make sense. "],["classification-with-k-nearest-neighbours-knn.html", "Module 3 Classification with K-Nearest Neighbours (KNN) 3.1 Key information 3.2 Advantages and Disadvantages of using the KNN model", " Module 3 Classification with K-Nearest Neighbours (KNN) The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another. 3.1 Key information A KNN model calculates similarity using the distance between two points on a graph. The greater the distance between the points, the less similar they are. There are multiple ways of calculating the distance between points, but the most common distance metric is just Euclidean distance (the distance between two points in a straight line). KNN is a supervised learning algorithm, meaning that the examples in the dataset must have labels assigned to them/their classes must be known. There are two other important things to know about KNN. First, KNN is a non-parametric algorithm. This means that no assumptions about the dataset are made when the model is used. Rather, the model is constructed entirely from the provided data. Second, there is no splitting of the dataset into training and test sets when using KNN. KNN makes no generalizations between a training and testing set, so all the training data is also used when the model is asked to make predictions. 3.2 Advantages and Disadvantages of using the KNN model 3.2.1 Advantages Easy to implement: Given the algorithm’s simplicity and accuracy, it is one of the first classifiers that a new data scientist will learn. Adapts easily: As new training samples are added, the algorithm adjusts to account for any new data since all training data is stored into memory. Few hyperparameters: KNN only requires a k value and a distance metric, which is low when compared to other machine learning algorithms. 3.2.2 Disadvantages Does not scale well: Since KNN is a lazy algorithm, it takes up more memory and data storage compared to other classifiers. This can be costly from both a time and money perspective. More memory and storage will drive up business expenses and more data can take longer to compute. While different data structures, such as Ball-Tree, have been created to address the computational inefficiencies, a different classifier may be ideal depending on the business problem. Curse of dimensionality: The KNN algorithm tends to fall victim to the curse of dimensionality, which means that it doesn’t perform well with high-dimensional data inputs. This is sometimes also referred to as the peaking phenomenon (PDF, 340 MB) (link resides outside of ibm.com), where after the algorithm attains the optimal number of features, additional features increases the amount of classification errors, especially when the sample size is smaller. Prone to overfitting: Due to the “curse of dimensionality”, KNN is also more prone to overfitting. While feature selection and dimensionality reduction techniques are leveraged to prevent this from occurring, the value of k can also impact the model’s behavior. Lower values of k can overfit the data, whereas higher values of k tend to “smooth out” the prediction values since it is averaging the values over a greater area, or neighborhood. However, if the value of k is too high, then it can underfit the data. 3.2.3 Things to guide you as you choose the value of K As K approaches 1, your prediction becomes less stable. As your value of K increases, your prediction becomes more stable. When you start receiving an increasing number of errors, you should know you are pushing your K too far. Taking a majority vote among labels needs K to be an odd number to have a tiebreaker. 3.2.4 Alternate distance metrics to Euclidean Minkowski distance Manhattan distance Hamming distance Cosine distance 3.2.5 KNN code example **PART 2: Data Modelling** **Question 7:** We will need to split our dataset into feature data (X) and target data (y). As discussed in class the feature data is the data that our machine learning model will *learn* in order to classify according to the target variable. These are: * sepal.length * sepal.width * petal.length * petal.width Extract the data for the first four columns and store it in a new dataframe called *iris_data_feature_data*. &quot;&quot;&quot; # Provide your solution here iris_data_feature_data = iris_data[[&#39;sepal.length&#39;,&#39;sepal.width&#39;,&#39;petal.length&#39;,&#39;petal.width&#39;]] iris_data_feature_data df1 = iris_data.iloc[:, 0:4] df1 &quot;&quot;&quot;**Question 8:** Now we will need to create the *iris_data_target_data* dataframe. The new dataframe will store only the target values.&quot;&quot;&quot; # Provide your solution here iris_data_target_data = iris_data[[&#39;variety&#39;]] iris_data_target_data &quot;&quot;&quot;**Question 9:** Study and run the following code.&quot;&quot;&quot; X_count = iris_data_feature_data.count() y_count = iris_data_target_data.count() print(&quot;X counts&quot;) print(X_count) print(&quot;y count&quot;) print(y_count) &quot;&quot;&quot;&gt; There are 150 rows for both *iris_data_feature_data* and *iris_data_target_data*. Note, that we cannot proceed with modeling if the counts do not match as we will receive an error! All X data should have the same number of rows as the y data. **Question 10:** As seen in class, create an X and y variable to represent iris_data_feature_data and iris_data_target_data respectively. Here, we just create two variables for representing X and y for our model. &quot;&quot;&quot; # Provide your solution here X = iris_data_feature_data y = iris_data_target_data &quot;&quot;&quot;**Question 11:** Create your KNN model and fit it using the X and y variables from the previous step. Use the *KNeighborsClassifier* including 4 neighbors.&quot;&quot;&quot; # Provide your solution here from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=4) knn.fit(X, y) &quot;&quot;&quot;&gt; The output of this script provides some details about the default setup of the KNN model. **Question 12:** What is the *metric=&#39;minkowski&#39;*? Look online and keep a note of your findings. &quot;&quot;&quot; # Provide your solution here # The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. # More details: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClass ifier.html &quot;&quot;&quot;**Question 13:** Provide a script to *predict* the class of a flower with the following data: * sepal.length = 4.5 * sepal.width = 3.2 * petal.length = 1.1 * petal.width = 0.4 &quot;&quot;&quot; # Examine the following code knn.predict([[4.5, 3.2, 1.1, 0.4]]) &quot;&quot;&quot;**Question 14:** Print the data for row 4 and compare with the prediction of the previous step. Then answer the questions.&quot;&quot;&quot; # Provide your solution here iris_data.loc[4] # Compare data from question 13 and data for row 4, Do they look similar? # Yes! Row:4 5.0 3.6 1.4 0.2 Setosa # Is the prediction of question 13 good? # Yes, data look close to each other! "],["classification-with-logistic-regression.html", "Module 4 Classification with Logistic Regression", " Module 4 Classification with Logistic Regression Logistic regression is a classification algorithm. It is used to predict a binary/ multinomial outcome based on a set of independent variables. The logistic regression model uses the Sigmoid function that is based on an estimated probability. 4.0.1 Logistic regression assumptions The dependent variable is binary or dichotomous—i.e. It fits into one of two clear-cut categories. This applies to binary logistic regression, which is the type of logistic regression we’ve discussed so far. We’ll explore some other types of logistic regression in section five. There should be no, or very little, multicollinearity between the predictor variables—in other words, the predictor variables (or the independent variables) should be independent of each other. This means that there should not be a high correlation between the independent variables. The independent variables should be linearly related to the log odds. Logistic regression requires fairly large sample sizes—the larger the sample size, the more reliable (and powerful) you can expect the results of your analysis to be. 4.0.2 Advantages and Disadvantages of using Logistic Regression 4.0.2.1 Advantages Logistic regression is much easier to implement than other methods, especially in the context of machine learning: Logistic regression is easier to train and implement as compared to other methods. Logistic regression works well for cases where the dataset is linearly separable: A dataset is said to be linearly separable if it is possible to draw a straight line that can separate the two classes of data from each other. Logistic regression is used when your Y variable can take only two values, and if the data is linearly separable, it is more efficient to classify it into two separate classes. Logistic regression provides useful insights: Logistic regression not only gives a measure of how relevant an independent variable is (i.e. the (coefficient size), but also tells us about the direction of the relationship (positive or negative). Two variables are said to have a positive association when an increase in the value of one variable also increases the value of the other variable. 4.0.2.2 Disadvantages Logistic regression fails to predict a continuous outcome. Logistic regression assumes linearity between the predicted (dependent) variable and the predictor (independent) variables. Why is this a limitation? In the real world, it is highly unlikely that the observations are linearly separable. Let’s imagine you want to classify the iris plant into one of two families: sentosa or versicolor. In order to distinguish between the two categories, you’re going by petal size and sepal size. You want to create an algorithm to classify the iris plant, but there’s actually no clear distinction—a petal size of 2cm could qualify the plant for both the sentosa and versicolor categories. So, while linearly separable data is the assumption for logistic regression, in reality, it’s not always truly possible. Logistic regression may not be accurate if the sample size is too small. If the sample size is on the small side, the model produced by logistic regression is based on a smaller number of actual observations. This can result in overfitting. In statistics, overfitting is a modeling error which occurs when the model is too closely fit to a limited set of data because of a lack of training data. Or, in other words, there is not enough input data available for the model to find patterns in it. In this case, the model is not able to accurately predict the outcomes of a new or future dataset. 4.0.3 Code example using Logistic Regression # **PART 1: Using Logistic Regression in the Iris dataset to predict a flower&#39;s variety (Classification problem)** **Question 1:** Load the *Iris.csv* file in the Colab and *read* it as a dataframe. &quot;&quot;&quot; # Provide your solution here import pandas as pd iris_data = pd.read_csv(&#39;Iris.csv&#39;) iris_data &quot;&quot;&quot;**Question 2:** Examine the data schema by printing the *types* of data for each column. Provide the answer for each question.&quot;&quot;&quot; # Provide your solution here iris_data.dtypes # How many columns do we have? __5__ # What type is the sepal.length column? __String__ &quot;&quot;&quot;**Question 3:** Continue examining the dataset, let us focus on the numerical data, columns include: * sepal.length -&gt; float64 * sepal.width -&gt; float64 * petal.length -&gt; float64 * petal.width -&gt; float64 Describe the dataset and answer the following questions &quot;&quot;&quot; # Provide your solution here iris_data.describe() &quot;&quot;&quot;**Question 4:** We will need to split our dataset into feature data **(X)** and target data **(y)**. We will use the sepal length and width, and the petal length and width to predict a flower&#39;s class (variety). So, the feature data is: * sepal.length * sepal.width * petal.length * petal.width The target data is: * variety Extract the data for the feature variables (and associated columns) and store it in a new dataframe called **iris_data_feature_data**. This will be our **X** variable. &quot;&quot;&quot; # Provide your solution here iris_data_feature_data = iris_data[[&#39;sepal.length&#39;,&#39;sepal.width&#39;,&#39;petal.length&#39;,&#39;petal.width&#39;]] X = iris_data_feature_data &quot;&quot;&quot;**Question 5:** Now we will need to create the **iris_data_feature_target** dataframe. The new dataframe will store only the target values that is the variety column. This will be our **y** variable.&quot;&quot;&quot; # Provide your solution here iris_data_feature_target = iris_data[[&#39;variety&#39;]] y = iris_data_feature_target &quot;&quot;&quot;**Question 6:** Create a new *LogisticRegression model* and then fit the **X** and **y** dataframes that we just created. Make sure you revise the class slides to create your model. **Do not worry about the warnings.** &quot;&quot;&quot; # Provide your solution here from sklearn.linear_model import LogisticRegression logreg = LogisticRegression(max_iter=200) logreg.fit(X,y) &quot;&quot;&quot;**Question 7:** Our model is now ready to use, test your model using the following data to predict a flower&#39;s class: **Set 1** * sepal.length: 4.5 * sepal.width: 3.2 * petal.length: 1.1 * petal.width: 0.4 **Set 2** * sepal.length: 6.9 * sepal.width: 3.0 * petal.length: 4.6 * petal.width: 1.5 **Set 3** * sepal.length: 6 * sepal.width: 3.5 * petal.length: 5.5 * petal.width: 2 &quot;&quot;&quot; # Provide your solution here # Prediction for set 1 print(logreg.predict([[4.5, 3.2, 1.1, 0.4]])) # Prediction for set 2 print(logreg.predict([[6.9, 3.0, 4.6, 1.5]])) # Prediction for set 3 print(logreg.predict([[6, 3.5, 5.5, 2]])) &quot;&quot;&quot;**Question 8:** What is the prediction of the KNN model for the same data sets? Revise last week&#39;s exercises if needed. Create your KNN model and fit it using the **X** and **y** variables from the previous step. Use the *KNeighborsClassifier* including 4 neighbors. &quot;&quot;&quot; # Provide your solution here from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=4) knn.fit(X, y) &quot;&quot;&quot;**Question 9:** Our KNN model is now ready to use, test your model using the following data to predict a flower&#39;s class (same as in question 7): **Set 1** * sepal.length: 4.5 * sepal.width: 3.2 * petal.length: 1.1 * petal.width: 0.4 **Set 2** * sepal.length: 6.9 * sepal.width: 3.0 * petal.length: 4.6 * petal.width: 1.5 **Set 3** * sepal.length: 6 * sepal.width: 3.5 * petal.length: 5.5 * petal.width: 2 &quot;&quot;&quot; # Provide your solution here # Prediction for set 1 print(knn.predict([[4.5, 3.2, 1.1, 0.4]])) # Prediction for set 2 print(knn.predict([[6.9, 3.0, 4.6, 1.5]])) # Prediction for set 3 print(knn.predict([[6, 3.5, 5.5, 2]])) &quot;&quot;&quot;**Question 10:** Compare the results between logistic regression and KNN.&quot;&quot;&quot; # We have the same or different predictions? # __Same__! "],["linear-polynomial-regression.html", "Module 5 Linear &amp; Polynomial Regression 5.1 Linear Regression 5.2 Assumptions 5.3 Classification vs Regression 5.4 Code example of Linear Regression 5.5 Polynomial Regression 5.6 Code example with Polynomial regression", " Module 5 Linear &amp; Polynomial Regression 5.1 Linear Regression Linear regression is a supervised learning algorithm that compares input (X) and output (Y) variables based on labeled data. It’s used for finding the relationship between the two variables and predicting future results based on past relationships. Completing a simple linear regression on a set of data results in a line on a plot representing the relationship between the independent variable X and the dependent variable Y. The simple linear regression predicts the value of the dependent variable based on the independent variable. 5.2 Assumptions Linearity and additivity: The expected value of the dependent variable is a straight-line function of the independent variable. The effects of different independent variables are additive to the expected value of the dependent variable. Check this using a scatterplot. No significant outliers: Outliers can have a negative effect on the regression analysis. Statistical independence: There is no correlation between consecutive errors when using time series data. The observations are independent of each other. Homoscedasticity: The errors have a constant variance in time compared to predictions and when compared to any independent variable. Normality: For a fixed value of X, Y values are distributed normally. 5.3 Classification vs Regression Differences: Regression output is continuous (numerical) Classification output is discrete (categorical) 5.4 Code example of Linear Regression **Building a linear regression model** We’ll import the numpy and pandas library in the Jupyter notebook and read the data using pandas. &quot;&quot;&quot; import numpy as np import pandas as pd # Read the given CSV file, and view some sample records advertising = pd.read_csv(&quot;Company_data.csv&quot;) &quot;&quot;&quot;The dataset looks like this. Here our target variable is the Sales column.&quot;&quot;&quot; advertising.head(10) &quot;&quot;&quot;Let’s perform some tasks to understand the data like shape, info, and describe.&quot;&quot;&quot; # Shape of our dataset advertising.shape # Info our dataset advertising.info() # Describe our dataset advertising.describe() &quot;&quot;&quot;Let’s now visualize the data using the matplolib and seaborn library. We’ll make a pairplot of all the columns and see which columns are the most correlated to Sales.&quot;&quot;&quot; # Import matplotlib and seaborn libraries to visualize the data import matplotlib.pyplot as plt import seaborn as sns # Using pairplot we&#39;ll visualize the data for correlation sns.pairplot(advertising, x_vars=[&#39;TV&#39;, &#39;Radio&#39;,&#39;Newspaper&#39;], y_vars=&#39;Sales&#39;, height=4, aspect=1, kind=&#39;scatter&#39;) plt.show() &quot;&quot;&quot;If we cannot determine the correlation using a scatter plot, we can use the seaborn heatmap to visualize the data. &quot;&quot;&quot; # Visualizing the data using heatmap sns.heatmap(advertising.corr(), cmap=&quot;YlGnBu&quot;, annot = True) plt.show() &quot;&quot;&quot;As we can see from the above graphs, the TV column seems most correlated to Sales. Let’s perform the simple linear regression model using TV as our feature variable. Equation of simple linear regression y = c + mX In our case: y = c + m * TV The m values are known as model coefficients or model parameters. We’ll perform simple linear regression in four steps. * Create X and y * Create Train and Test set * Train your model * Evaluate the model The independent variable represents X, and y represents the target variable in a simple linear regression model. &quot;&quot;&quot; X = advertising[[&#39;TV&#39;]] y = advertising[[&#39;Sales&#39;]] &quot;&quot;&quot;We need to split our variables into training and testing sets. Using the training set, we’ll build the model and perform the model on the testing set. We’ll divide the training and testing sets into a 7:3 ratio, respectively. We’ll split the data by importing train_test_split from the sklearn.model_selection library. &quot;&quot;&quot; from sklearn.linear_model import LinearRegression regressor = LinearRegression() regressor.fit(X,y) &quot;&quot;&quot;To retrieve the intercept: &quot;&quot;&quot; print(regressor.intercept_) &quot;&quot;&quot;For retrieving the slope (coefficient of x): &quot;&quot;&quot; print(regressor.coef_) &quot;&quot;&quot;We can conlude that our model is: * y = 6.97 + 0.05 * TV That means that if we use TV=100, then: * y = 6.97 + 0.05 * 100 = 6.97 + 5 = 11.97 Lets extract the first record e.g. record 0 &quot;&quot;&quot; advertising.head(1) &quot;&quot;&quot;And now, let&#39;s try to see the prediction for **TV value: 230.1**, as we can see our prediction is 19.73, that is close to 22.1&quot;&quot;&quot; y_pred = regressor.predict([[230.1]]) y_pred 5.5 Polynomial Regression Polynomial regression is a form of linear regression in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial. Polynomial Regression is a form of regression analysis in which the relationship between the independent variables and dependent variables are modelled in the nth degree polynomial. Polynomial Regression models are usually fitted with the method of least squares. The least-square method minimizes the variance of the coefficients, under the Gauss Markov Theorem. Polynomial Regression is a special case of Linear Regression where we fit the polynomial equation on the data with a curvilinear relationship between the dependent and independent variables. 5.6 Code example with Polynomial regression # **PART 2: Using Polynomial Regression** **Question 1.1:** Load the *Years_salary_data.csv* file in the Colab and *read* it as a dataframe. Print and examine the dataset. &quot;&quot;&quot; # Provide your solution here import pandas as pd data_ys = pd.read_csv(&#39;Years_salary_data.csv&#39;) data_ys.head() &quot;&quot;&quot;**Question 1.2:** Let&#39;s plot a scatterplot to examine the data.&quot;&quot;&quot; sns.scatterplot(x=&#39;Years&#39;, y=&#39;Salary&#39;, data=data_ys) &quot;&quot;&quot;**Question 2:** Plot the years-salary data using a lmplot. Use in **x-axis** the **Years** and as **y-axis** the **Salary**. &quot;&quot;&quot; # Provide your solution here sns.lmplot(x=&#39;Years&#39;, y=&#39;Salary&#39;, data=data_ys, height=10) &quot;&quot;&quot;**Question 3:** Create the **X (Years)** and **y (Salary)** variables of your dataset.&quot;&quot;&quot; # Provide your solution here X= data_ys[[&#39;Years&#39;]] y= data_ys[[&#39;Salary&#39;]] &quot;&quot;&quot;**Question 4:** Create the Linear Regression model. Fit **X** and **y**. &quot;&quot;&quot; # Provide your solution here from sklearn.linear_model import LinearRegression lin = LinearRegression() lin.fit(X, y) &quot;&quot;&quot;**Question 5:** Create the Polynomial Regression model (degree=8). Fit X and y. Study and run the following code. &quot;&quot;&quot; # Fitting Polynomial Regression to the dataset from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree = 8) X_poly = poly.fit_transform(X) poly.fit(X_poly, y) lin2 = LinearRegression() lin2.fit(X_poly, y) &quot;&quot;&quot;**Question 6:** Plot the Linear Regression model. Study and run the following code. Is the Regression line a good fit? &quot;&quot;&quot; # Visualising the Linear Regression results plt.scatter(X, y, color = &#39;blue&#39;) plt.plot(X, lin.predict(X), color = &#39;red&#39;) plt.title(&#39;Linear Regression&#39;) plt.xlabel(&#39;Temperature&#39;) plt.ylabel(&#39;Pressure&#39;) plt.show() # The regression line does not represent the data &quot;&quot;&quot;**Question 7:** Plot the Polynomial Regression results. Study and run the following code. Is the Polynomial line a good fit? &quot;&quot;&quot; # Visualising the Polynomial Regression results plt.scatter(X, y, color = &#39;blue&#39;) plt.plot(X, lin2.predict(poly.fit_transform(X)), color = &#39;red&#39;) plt.title(&#39;Polynomial Regression&#39;) # The Polynomial line represent better the data! &quot;&quot;&quot;**Question 8:** Let us change the degree of the Polynomial line. Study and run the following code using the following. * degree=4 * degree=10 &quot;&quot;&quot; # Fitting Polynomial Regression to the dataset from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree = 10) X_poly = poly.fit_transform(X) poly.fit(X_poly, y) lin2 = LinearRegression() lin2.fit(X_poly, y) # Visualising the Polynomial Regression results plt.scatter(X, y, color = &#39;blue&#39;) plt.plot(X, lin2.predict(poly.fit_transform(X)), color = &#39;red&#39;) plt.title(&#39;Polynomial Regression&#39;) &quot;&quot;&quot;&gt; The degree affects the fit of our Polynomial line, if the degree is very small our model does not fit well to the data, if the degree is very high the model overfits the data. We need to find a balance to control it. **Question 9:** Create a Polynomial Regression model using degree 11. Use the code from the previous examples. &quot;&quot;&quot; # Provide your solution here from sklearn.preprocessing import PolynomialFeatures poly_reg = PolynomialFeatures(degree=11) X_poly = poly_reg.fit_transform(X) pol_reg = LinearRegression() pol_reg.fit(X_poly, y) &quot;&quot;&quot;**Question 10:** Predict the salary for years of experience as follows. * 32.5 * 20.5 Create a linear and Polynomial Regression prediction. Study and run the following code. Which model is more accurate? &quot;&quot;&quot; # Predicting a new result with Linear Regression print(lin.predict([[20.5]])) # Predicting a new result with Polymonial Regression print(pol_reg.predict(poly_reg.fit_transform([[20.5]]))) # Answer: __Polynomial Regression is a better fit model in this dataset___ "],["probabilistic-classifiers-and-gradient-descent.html", "Module 6 Probabilistic Classifiers and Gradient Descent 6.1 Naive Bayes Classification 6.2 Types of Naive Bayes Classifier 6.3 Advantages and Disadvantages of Naive Bayes Classifers 6.4 Multinomial Naive Bayes 6.5 Gradient Descent", " Module 6 Probabilistic Classifiers and Gradient Descent 6.1 Naive Bayes Classification Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. \\[ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\] ** Learning Rate ** The size of these steps is called the learning rate. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom. ** Cost Function ** A Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate. 6.2 Types of Naive Bayes Classifier Multinomial Naive Bayes: This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document. Bernoulli Naive Bayes: This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not. Gaussian Naive Bayes: When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution. 6.3 Advantages and Disadvantages of Naive Bayes Classifers 6.3.1 Advantages Simple to Implement. The conditional probabilities are easy to evaluate. Very fast – no iterations since the probabilities can be directly computed. So this technique is useful where speed of training is important. Good classification results - If the conditional Independence assumption holds, it could give great results. 6.3.2 Disadvantages Assumptions for class conditionals are independence Practically, dependencies exist among variables which cannot be modelled by Naive Bayes. e.g. patient profile with symptoms. 6.4 Multinomial Naive Bayes Multinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output. 6.5 Gradient Descent Gradient descent is an optimization algorithm that’s used when training a machine learning model. It’s based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. 6.5.1 Types of Gradient Descent 6.5.1.1 Batch Gradient Descent Batch gradient descent sums the error for each point in a training set, updating the model only after all training examples have been evaluated. This process referred to as a training epoch. While this batching provides computation efficiency, it can still have a long processing time for large training datasets as it still needs to store all of the data into memory. Batch gradient descent also usually produces a stable error gradient and convergence, but sometimes that convergence point isn’t the most ideal, finding the local minimum versus the global one. 6.5.1.1.1 Advantages More stable convergence and error gradient than Stochastic Gradient descent Embraces the benefits of vectorization A more direct path is taken towards the minimum Computationally efficient since updates are required after the run of an epoch 6.5.1.1.2 Disadvantages Can converge at local minima and saddle points Slower learning since an update is performed only after we go through all observations 6.5.1.2 Stochastic Gradient Descent Stochastic gradient descent (SGD) runs a training epoch for each example within the dataset and it updates each training example’s parameters one at a time. Since you only need to hold one training example, they are easier to store in memory. While these frequent updates can offer more detail and speed, it can result in losses in computational efficiency when compared to batch gradient descent. Its frequent updates can result in noisy gradients, but this can also be helpful in escaping the local minimum and finding the global one. 6.5.1.2.1 Advantages Only a single observation is being processed by the network so it is easier to fit into memory May (likely) to reach near the minimum (and begin to oscillate) faster than Batch Gradient Descent on a large dataset The frequent updates create plenty of oscillations which can be helpful for getting out of local minimums. 6.5.1.2.2 Disadvantages Can veer off in the wrong direction due to frequent updates Lose the benefits of vectorization since we process one observation per time Frequent updates are computationally expensive due to using all resources for processing one training sample at a time 6.5.1.3 Mini-batch Gradient Descent Mini-batch gradient descent combines concepts from both batch gradient descent and stochastic gradient descent. It splits the training dataset into small batch sizes and performs updates on each of those batches. This approach strikes a balance between the computational efficiency of batch gradient descent and the speed of stochastic gradient descent. 6.5.1.3.1 Advantages Convergence is more stable than Stochastic Gradient Descent Computationally efficient Fast Learning since we perform more updates 6.5.1.3.2 Disadvantages We have to configure the Mini-Batch size hyperparameter "],["model-evaluation-for-regression-and-classification.html", "Module 7 Model Evaluation for Regression and Classification 7.1 What is Model Evaluation? 7.2 Methods 7.3 Measuring Model Performance", " Module 7 Model Evaluation for Regression and Classification 7.1 What is Model Evaluation? It helps to find the best model that represents our data and how well the chosen model will work in the future. It is the process through which we quantify the quality of a systems predictions. We measure the trained model performance on the new and independent dataset. Generalisation refers to the models ability to adapt properly to nw and unseen data, drawn from the same distribution as the one used to create the model. 7.2 Methods Train and test Train the model on the entire dataset Test the model on the same dataset This shows us how well the model performed on same data We can check if the predictions were actually correct or not Train - Test - Split Test to make predictions on data that you do not use to train the model Since our training model uses part of our dataset the testing phase is more realistic Better simulation how a model is likely to perform on out of sample data We have the real values of testing data so we can check how the model performed 7.2.1 Issues with train-test-split The results may be highly bias if a subset of our dataset is skewed towards a certain group or class. This results in overfitting which we are trying to avoid. 7.2.2 Cross validation Similar to train/test split but applied to more subsets of data. Splits data into k subsets and train on k-1 of those subsets. Use the last subset for testing Process repeated for each subset 7.3 Measuring Model Performance 7.3.1 Variance and sensitivity Variance is a type of error that occurs due to a models sensitivity to small fluctations in the training set. High variance would cause and algorithm to model the noise in the training set aka overfitting. We need to find the balance between variance and sensitivity (low bias and low variance) High bias can cause the algorithm to miss relationships between features and target outputs. 7.3.2 Confusion Matrix A Confusion Matrix is a performance measurement for a statistical learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values. True Positive is an outcome where the model correctly predicts a positive class. True Negative is an outcome where the model correctly predicts the negative class. False Positive is an outcome where the model incorrectly predicts the positive outcome. False Negative is an outcome where the model incorrectly predicts the negative outcome. ###Sensitivity vs Specificity Sensitivity is the metric that evaluates a models ability to predict true positives of each category. \\[ Sensitivity = \\frac{True Positives}{True Positives + False Negatives} \\] Specificity is the metric that evaluates a models ability to predict true negatives of each category. \\[ Specificity = \\frac{True Negatives}{True Negatives + False Positives} \\] "],["advanced-statistical-learning-models.html", "Module 8 Advanced Statistical Learning Models 8.1 Unsupervised learning", " Module 8 Advanced Statistical Learning Models 8.1 Unsupervised learning Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervention. Its ability to discover similarities and differences in information make it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition. The data has no target attribute Explore the data to find intrinsic structures in them. Grouping similar instances together. Draws inferences from datasets without labels Best if you arent sure what the target variable should be. We can use Clustering to group items together based on similiarity. 8.1.1 Clustering Clustering is a data mining technique that groups unlabeled data based on their similarities or differences. Clustering algorithms are used to process raw, unclassified data objects into groups represented by structures or patterns in the information. Clustering algorithms can be categorized into a few types: exclusive, overlapping, hierarchical, and probabilistic. 8.1.1.1 K-Means Clustering K-Means is a partitional clustering algorithm Clustering is a technique for finding similarity groups in data called clusters. Groups data instances that are similar to each other in one cluster and data instances that are very different from each other into different clusters. K-Means is very efficient at clustering Tries to find the centre of each cluster and assign each instance to the closest cluster. 8.1.1.1.1 How does it work? Randomly choose k data points to be the initial centroids, cluster centres. Assign each data point to the closest centroid Re-compute the centroids using the current cluster memberships If a convergence criterion is not met repeat step 2. When does the model stop? No or min re-assignments of data points to the different clusters. No or min change of centroids Minimum decrease in the sum of squared error (SSE) 8.1.1.1.2 Advantages and Disadvantages 8.1.1.1.2.1 Advantages It is simple, highly flexible, and efficient. The simplicity of k-means makes it easy to explain the results in contrast to Neural Networks. The flexibility of k-means allows for easy adjustment if there are problems. The efficiency of k-means implies that the algorithm is good at segmenting a dataset. An instance can change cluster (move to another cluster) when the centroids are recomputed Easy to interpret the clustering results. 8.1.1.1.2.2 Disadvantages It does not allow to develop the most optimal set of clusters and the number of clusters must be decided before the analysis. How many clusters to include is left at the discretion of the researcher. This involves a combination of common sense, domain knowledge, and statistical tools. Too many clusters tell you nothing because of the groups becoming very small and there are too many of them. There are statistical tools that measure within-group homogeneity and group heterogeneity. There are methods like the elbow method to decide the value of k. Additionally, there is a technique called a dendrogram. The results of a dendrogram analysis provide a recommendation of how many clusters to use. However, calculating a dendrogram for a large dataset could potentially crash a computer due to the computational load and the limits of RAM. When doing the analysis, the k-means algorithm will randomly select several different places from which to develop clusters. This can be good or bad depending on where the algorithm chooses to begin at. From there, the centre of the clusters is recalculated until an adequate “centre’’ is found for the number of clusters requested. The order of the data has an impact on the final results. 8.1.2 Decision Trees A decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails). Each leaf/node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from the root to the leaf represent classification rules. 8.1.3 Principal Component Analysis Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. 8.1.4 Support Vector Machine Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called support vectors, and hence the algorithm is termed a Support Vector Machine. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane "],["relational-database-systems.html", "Module 9 Relational Database Systems 9.1 SQLite", " Module 9 Relational Database Systems 9.1 SQLite SQLite is a popular open source SQL database. It can store an entire database in a single file. One of the most significant advantages this provides is that all of the data can be stored locally without having to connect your database to a server. SQLite is not directly comparable to client/server SQL database engines such as MySQL, Oracle, PostgreSQL, or SQL Server since SQLite is trying to solve a different problem. Client/server SQL database engines strive to implement a shared repository of enterprise data. They emphasize scalability, concurrency, centralization, and control. SQLite strives to provide local data storage for individual applications and devices. SQLite emphasizes economy, efficiency, reliability, independence, and simplicity 9.1.1 Advantages and Disadvantages 9.1.1.1 Advantages Lightweight database and easy to implement Good community support online Simple to use 9.1.1.2 Disadvantages Not scalable Not suitable for large applications Does not support concurrent transactions on data Speed to query may be reduced if you have a large data set "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
